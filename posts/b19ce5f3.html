<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>超算暑假log | Murmure's Blog</title><meta name="keywords" content="JNU,超算"><meta name="author" content="Murmure"><meta name="copyright" content="Murmure"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="超算暑假log记录">
<meta property="og:type" content="article">
<meta property="og:title" content="超算暑假log">
<meta property="og:url" content="http://blog.ifycyu.ml/posts/b19ce5f3.html">
<meta property="og:site_name" content="Murmure&#39;s Blog">
<meta property="og:description" content="超算暑假log记录">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://p9.itc.cn/images01/20200619/bbf273ea4df24f0b938fd64d2cf039bd.png">
<meta property="article:published_time" content="2022-07-10T16:00:00.000Z">
<meta property="article:modified_time" content="2022-08-22T10:15:18.067Z">
<meta property="article:author" content="Murmure">
<meta property="article:tag" content="JNU">
<meta property="article:tag" content="超算">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://p9.itc.cn/images01/20200619/bbf273ea4df24f0b938fd64d2cf039bd.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://blog.ifycyu.ml/posts/b19ce5f3"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="manifest" href="/"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="/pluginsSrc/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="/pluginsSrc/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: {"limitDay":30,"position":"top","messagePrev":"已经","messageNext":"天没有更新了。文章内容可能已经过时了"},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'mediumZoom',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: '/pluginsSrc/flickr-justified-gallery/dist/fjGallery.min.js',
      css: '/pluginsSrc/flickr-justified-gallery/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '超算暑假log',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-08-22 18:15:18'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/iconfont.css"><link rel="stylesheet" href="/css/background.css"><link rel="stylesheet" href="/css/custom/twikoo_beautify.css"  media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="iron-container iron-circle"><div class="iron-box1 iron-circle iron-center"></div><div class="iron-box2 iron-circle iron-center"></div><div class="iron-box3 iron-circle iron-center"></div><div class="iron-box4 iron-circle iron-center"></div><div class="iron-box5 iron-circle iron-center"></div><div class="iron-box6 iron-circle"><div class="iron-coil" style="--i: 0"></div><div class="iron-coil" style="--i: 1"></div><div class="iron-coil" style="--i: 2"></div><div class="iron-coil" style="--i: 3"></div><div class="iron-coil" style="--i: 4"></div><div class="iron-coil" style="--i: 5"></div><div class="iron-coil" style="--i: 6"></div><div class="iron-coil" style="--i: 7"></div></div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">23</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-book"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-coffee"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/artitalk/"><i class="fa-fw fa fa-comments"></i><span> 说说</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li><li><a class="site-page child" href="/books/"><i class="fa-fw fas fa-book"></i><span> 书</span></a></li><li><a class="site-page child" href="/trips/"><i class="fa-fw fas fa-map"></i><span> 旅行</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fas fa-computer"></i><span> 游戏</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 相册</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-address-card"></i><span> 社交</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-user"></i><span> 关于我</span></a></li><li><a class="site-page child" href="/contact/"><i class="fa-fw fas fa-comment-dots"></i><span> 留言</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('http://p9.itc.cn/images01/20200619/bbf273ea4df24f0b938fd64d2cf039bd.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Murmure's Blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-book"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-coffee"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/artitalk/"><i class="fa-fw fa fa-comments"></i><span> 说说</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li><li><a class="site-page child" href="/books/"><i class="fa-fw fas fa-book"></i><span> 书</span></a></li><li><a class="site-page child" href="/trips/"><i class="fa-fw fas fa-map"></i><span> 旅行</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fas fa-computer"></i><span> 游戏</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 相册</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-address-card"></i><span> 社交</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-user"></i><span> 关于我</span></a></li><li><a class="site-page child" href="/contact/"><i class="fa-fw fas fa-comment-dots"></i><span> 留言</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">超算暑假log</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-07-10T16:00:00.000Z" title="发表于 2022-07-11 00:00:00">2022-07-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-08-22T10:15:18.067Z" title="更新于 2022-08-22 18:15:18">2022-08-22</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/JNU/">JNU</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/JNU/%E8%B6%85%E7%AE%97/">超算</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">13.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>49分钟</span></span><span class="post-meta-separator">|</span><span id="" data-flag-title="超算暑假log"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="twikoo_visitors"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><meta name="referrer" content="no-referrer">

<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>基本是有关科学计算的,AI方面的好少…</p>
<h1 id="7月11日"><a href="#7月11日" class="headerlink" title="7月11日"></a>7月11日</h1><h2 id="大纲"><a href="#大纲" class="headerlink" title="大纲"></a>大纲</h2><ol>
<li><p>复习linux命令（大二下学期选修linux）</p>
</li>
<li><p>学习编译器编译过程</p>
</li>
<li><p>了解make cmake ，makefile文件 CmakeList.txt文件</p>
</li>
<li><p>掌握c、c++、python</p>
</li>
<li><p>cpu架构，指令集，gpu架构</p>
</li>
</ol>
<h2 id="linux基本命令"><a href="#linux基本命令" class="headerlink" title="linux基本命令:"></a>linux基本命令:</h2><ol>
<li>cd 切换目录</li>
<li>pwd 获取绝对路径</li>
<li>touch 更新文件时间标签,文件不存在则创建文件</li>
<li>mkdir 创建目录</li>
<li>cp 复制文件或目录</li>
<li>mv 移动文件或目录</li>
<li>rm 删除文件或目录(rm -rf ..)</li>
<li>rmdir 删除目录</li>
<li>cat 查看文件</li>
<li>more 分页查看文件</li>
<li>head/tail -n 查看文件前/后n行</li>
<li>chmod 修改权限(a,u,g,o)(+-=)/rwx对应421 </li>
<li>chown 修改所有者  chown user[:group] file</li>
<li>which 查看命令所在路径</li>
<li>grep 搜索与匹配</li>
<li>ifconfig 网络参数</li>
<li>gzip/gunzip (.gz) zip/unzip(.zip) tar   (tar -zxvf xxx.tar.gz)</li>
<li>man COMMAND 帮助文档</li>
<li>top 显示进程相关信息</li>
<li>ps -ef | grep xxxx</li>
<li>ls &gt; /xxx  输出重定向  (/dev/null 黑洞)</li>
</ol>
<h2 id="编译器编译过程"><a href="#编译器编译过程" class="headerlink" title="编译器编译过程"></a>编译器编译过程</h2><ol>
<li>预处理         文件合并、宏定义替换、删除注释等    由.c文件到.i文件</li>
<li>编译             将高级语言源代码转换为汇编代码      由.i文件到.s文件</li>
<li>汇编             生成可重定位的机器码                        由.s文件到.o文件</li>
<li>链接             将程序所引用的外部文件关联起来      由.o文件到可执行文件</li>
</ol>
<h2 id="make-cmake-makefile-CmakeList-txt"><a href="#make-cmake-makefile-CmakeList-txt" class="headerlink" title="make cmake makefile CmakeList.txt"></a>make cmake makefile CmakeList.txt</h2><p>makefile格式</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;target&gt; : &lt;prerequisites&gt; </span><br><span class="line">[tab]  &lt;commands&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">cal:main.o add.o sub.o mul.o</span></span><br><span class="line">	gcc main.o add.o sub.o mul.o -o cal</span><br><span class="line"></span><br><span class="line"><span class="section">main.o:main.c</span></span><br><span class="line">	gcc -c main.c -o main.o</span><br><span class="line"><span class="section">add.o:add.c</span></span><br><span class="line">	gcc -c add.c -o add.o</span><br><span class="line"><span class="section">sub.o:sub.c</span></span><br><span class="line">	gcc -c sub.c -o sub.o</span><br><span class="line"><span class="section">mul.o:mul.c</span></span><br><span class="line">	gcc -c mul.c -o mul.o</span><br></pre></td></tr></table></figure>
<figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">src = <span class="variable">$(<span class="built_in">wildcard</span> *.c)</span> <span class="comment"># 获取工作目录下所有.c文件列表</span></span><br><span class="line">obj = <span class="variable">$(<span class="built_in">patsubst</span> %.c, %.o, <span class="variable">$(src)</span>)</span> <span class="comment"># 模式字符串替换(符合xxx.c替换成xxx.o)</span></span><br><span class="line"></span><br><span class="line"><span class="section">ALL:cal</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$(obj)</span>:%.o:%.c</span><br><span class="line">	gcc -c <span class="variable">$&lt;</span> -o <span class="variable">$@</span> <span class="comment"># $&lt; 依赖文件第一个</span></span><br><span class="line"></span><br><span class="line"><span class="section">cal:<span class="variable">$(obj)</span></span></span><br><span class="line">	gcc <span class="variable">$^</span> -o <span class="variable">$@</span> <span class="comment"># $^ 所有的依赖文件</span></span><br><span class="line"></span><br><span class="line"><span class="section">clean:</span></span><br><span class="line">	-rm -rf <span class="variable">$(obj)</span> cal</span><br></pre></td></tr></table></figure>


<p>CMakeLists.txt</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># CMake最低版本要求</span><br><span class="line">cmake_minimum_required(VERSION 3.5)</span><br><span class="line"></span><br><span class="line"># 项目名称</span><br><span class="line">project(test_1)</span><br><span class="line"></span><br><span class="line"># 包含子目录头文件</span><br><span class="line">include_directories(&quot;$&#123;PROJECT_SOURCE_DIR&#125;&#x2F;common&quot;)</span><br><span class="line"></span><br><span class="line"># 添加链接静态库</span><br><span class="line">target_link_libraries(mul_cmake_demo CommonFuncs)</span><br><span class="line"></span><br><span class="line"># 将.&#x2F;src 下的所有文件名保存到DIR_SRCS变量</span><br><span class="line">aux_source_directory(.&#x2F;src DIR_SRCS)</span><br><span class="line"></span><br><span class="line"># 设置变量为多个给定的值</span><br><span class="line">set(normal_var a b c)</span><br><span class="line"></span><br><span class="line"># 生成可执行文件，test_1是可执行文件的名字，hello.c是源文件名称，如有其他源文件，可在后面添加</span><br><span class="line">add_executable(test_1 hello.c) </span><br></pre></td></tr></table></figure>


<h2 id="c、c-、python"><a href="#c、c-、python" class="headerlink" title="c、c++、python"></a>c、c++、python</h2><h2 id="指令集，gpu架构"><a href="#指令集，gpu架构" class="headerlink" title="指令集，gpu架构"></a>指令集，gpu架构</h2><h3 id="指令集"><a href="#指令集" class="headerlink" title="指令集"></a>指令集</h3><p>指令集，是CPU中用来计算和控制计算机系统的一套指令的集合。CPU依靠指令来计算和控制系统，指令执行能力是衡量CPU性能的重要指标，指令集也与CPU效率有密切关系。CPU都有一个基本的指令集</p>
<p>AVX</p>
<p>​    AVX指令集遵循IEEE-754规范，其中的32-bit浮点数为单精度浮点数（float），64-bit浮点数为双精度浮点数（double）。因为AVX中的YMM寄存器为256-bit，所以若存储单精度浮点数32-bit，可存储8个，若存储双精度浮点数64-bit，可存储4个。</p>
<p>AVX2</p>
<p>​    整数和浮点计算理论性能又翻倍</p>
<p>AVX512</p>
<p>​    AVX512可用于整数和浮点运算中，并不是只能提高浮点性能，对于整数性能也是可以提高的，整数运算大约提升33%的理论性能。浮点方面，目前支持AVX512的处理器最高可实现双发射AVX512FMA运算，浮点峰值可以达到双发射AVX256FMA的一倍</p>
<h3 id="gpu架构"><a href="#gpu架构" class="headerlink" title="gpu架构"></a>gpu架构</h3><p>GPU的任务是天然并行的，现代GPU的架构皆是以高度并行能力而设计的。</p>
<h3 id="cuda-core"><a href="#cuda-core" class="headerlink" title="cuda core"></a>cuda core</h3><p>​    cuda core是可以执行 32 位浮点加法、32 位浮点乘法、32 位到 8 位整数运算（如移位、加法、muls 和类似操作）的管道（pipe），并且还可以在其 SM 单元上生成内存请求以提供数据以不断处理数据 . 还要求特殊函数计算并由 SM 单元同步，以正确计算并行算法。</p>
<h3 id="tensor-core"><a href="#tensor-core" class="headerlink" title="tensor core"></a>tensor core</h3><p>Tensor Cores支持混合精度计算，动态调整计算以加快吞吐量，同时保持精度。</p>
<h1 id="7月12日"><a href="#7月12日" class="headerlink" title="7月12日"></a>7月12日</h1><h2 id="大纲-1"><a href="#大纲-1" class="headerlink" title="大纲"></a>大纲</h2><ol>
<li>学习torch.nn</li>
<li>数据集处理部分</li>
</ol>
<h3 id="pytorch"><a href="#pytorch" class="headerlink" title="pytorch"></a>pytorch</h3><h4 id="torch-nn"><a href="#torch-nn" class="headerlink" title="torch.nn"></a>torch.nn</h4><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/HiWangWenBing/article/details/120614234">torch.nn库五大基本功能</a></p>
<h5 id="nn-Linear类（全连接层）"><a href="#nn-Linear类（全连接层）" class="headerlink" title="nn.Linear类（全连接层）"></a>nn.Linear类（全连接层）</h5><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img-blog.csdnimg.cn/20211005165134867.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5paH54Gr5Yaw57OW55qE56GF5Z-65bel5Z2K,size_20,color_FFFFFF,t_70,g_se,x_16"></p>
<h5 id="nn-functional（常见函数"><a href="#nn-functional（常见函数" class="headerlink" title="nn.functional（常见函数)"></a>nn.functional（常见函数)</h5><p>nn.functional包括神经网络前向和后向处理所需要到的常见函数。</p>
<h5 id="nn-xxx和nn-functional-xxx比较"><a href="#nn-xxx和nn-functional-xxx比较" class="headerlink" title="nn.xxx和nn.functional.xxx比较"></a>nn.xxx和nn.functional.xxx比较</h5><p><code>import torch.nn.functional as F</code> 包含<code>loss</code>和<code>activation function</code></p>
<p><code>nn.Xxx</code> 继承于 <code>nn.Module</code>，能够很好的与 <code>nn.Sequential</code> 结合使用</p>
<p>而 <code>nn.functional.xxx</code> 无法与 <code>nn.Sequential</code> 结合使用</p>
<p>eg:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.BatchNorm2d(num_features=<span class="number">64</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">2</span>),</span><br><span class="line">    nn.Droput(<span class="number">0.2</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h5 id="nn-Module类"><a href="#nn-Module类" class="headerlink" title="nn.Module类"></a>nn.Module类</h5><p>可以表示神经网络中的某个层,也可以表示一个包含很多层的神经网络</p>
<h5 id="自定义神经网络模型类（继承于Module类）"><a href="#自定义神经网络模型类（继承于Module类）" class="headerlink" title="自定义神经网络模型类（继承于Module类）"></a>自定义神经网络模型类（继承于Module类）</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NetC</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># 定义神经网络</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NetC, self).__init__()</span><br><span class="line">        self.h1 = nn.Linear(n_feature, n_hidden)</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line">        self.out = nn.Linear(n_hidden, n_output)</span><br><span class="line">        self.softmax = nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="comment">#定义前向运算</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 得到的数据格式torch.Size([64, 1, 28, 28])需要转变为（64,784）</span></span><br><span class="line">        x = x.view(x.size()[<span class="number">0</span>],-<span class="number">1</span>) <span class="comment"># -1表示自动匹配</span></span><br><span class="line">        h1 = self.h1(x)</span><br><span class="line">        a1 =  self.relu1(h1)</span><br><span class="line">        out = self.out(a1)</span><br><span class="line">        a_out = self.softmax(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>




<h4 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># You should build your custom dataset as below.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomDataset</span>(<span class="params">torch.utils.data.Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># TODO</span></span><br><span class="line">        <span class="comment"># 1. Initialize file paths or a list of file names. </span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="comment"># TODO</span></span><br><span class="line">        <span class="comment"># 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).</span></span><br><span class="line">        <span class="comment"># 2. Preprocess the data (e.g. torchvision.Transform).</span></span><br><span class="line">        <span class="comment"># 3. Return a data pair (e.g. image and label).</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># You should change 0 to the total size of your dataset.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span> </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment">## eg.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">My_dataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.x = torch.randn(<span class="number">1000</span>,<span class="number">3</span>)</span><br><span class="line">        self.y = self.x.<span class="built_in">sum</span>(axis=<span class="number">1</span>)</span><br><span class="line">        self.src,  self.trg = [], []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">            self.src.append(self.x[i])</span><br><span class="line">            self.trg.append(self.y[i])</span><br><span class="line">           </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.src[index], self.trg[index]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.src) </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="Dataloader"><a href="#Dataloader" class="headerlink" title="Dataloader"></a>Dataloader</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DataLoader(dataset, batch_size=<span class="number">1</span>, shuffle=<span class="literal">False</span>, sampler=<span class="literal">None</span>,</span><br><span class="line">           batch_sampler=<span class="literal">None</span>, num_workers=<span class="number">0</span>, collate_fn=<span class="literal">None</span>,</span><br><span class="line">           pin_memory=<span class="literal">False</span>, drop_last=<span class="literal">False</span>, timeout=<span class="number">0</span>,</span><br><span class="line">           worker_init_fn=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>dataset 很重要，需要将dataset定义好，完成相关函数的重写。</p>
<p>batch_size 就是训练的一个批次的样本数。</p>
<p>shuffle 表示每一个epoch中训练样本的顺序是否相同，一般True。</p>
<p>sampler 采样器</p>
<p>batch_sampler 将前面的Sampler采样得到的索引值进行合并，当数量等于一个batch大小后就将这一批的索引值返回。</p>
<p>num_workers 表示同时参与数据读取的线程数量，多线程技术可以加快数据读取，提供GPU/CPU利用率。</p>
<h1 id="7月13日"><a href="#7月13日" class="headerlink" title="7月13日"></a>7月13日</h1><h2 id="大纲-2"><a href="#大纲-2" class="headerlink" title="大纲:"></a>大纲:</h2><ol>
<li>集群</li>
<li>编译器优化</li>
</ol>
<h2 id="集群"><a href="#集群" class="headerlink" title="集群"></a>集群</h2><p>​    计算机集群(称为集群)是高度紧密协调的系统，可以通过一组松散集成的计算机软件和/或硬件来完成计算工作。</p>
<p>4个PC通过交换机连接在一起 NIC表示网络接口,PCI表示I/O总线,</p>
<p>如果将图中的交换机换为共享磁盘,可以得到共享磁盘的集群系统结构</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/07/13/e4k91.png"></p>
<p>集群的每台计算机都称为节点</p>
<p>集群系统功耗：各个部件的功耗加在一起</p>
<p>SSD代替HDD可以减低系统功耗</p>
<p>可以将集群的安装和配置步骤做成脚本，或者实现将系统安装在SSD上</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/zhangnianli127/article/details/10193139">如何设置集群的各个节点之间无密码登录</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://wenku.baidu.com/view/1a5313c7e518964bce847c3e.html">mpi实验 虚拟机环境下两节点的MPI集群搭建)</a></p>
<h2 id="编译器优化"><a href="#编译器优化" class="headerlink" title="编译器优化"></a>编译器优化</h2><p>优化编译器就是要消除简单语言翻译中可能引入的低效率，改进目标程序的性能。</p>
<p>性能分析引导优化(Profile Guided Optimization)通过分析程序运行时的实际行为，将结果反馈给编译器，使得编译器可以重新安排代码以减少指令缓存问题和分支预测误判，从而获得性能的提升。</p>
<h3 id="Intel-MPI"><a href="#Intel-MPI" class="headerlink" title="Intel-MPI"></a>Intel-MPI</h3><p>​    英特尔® MPI 库是一个实现开源 MPICH 规范的多结构消息传递库。使用该库创建、维护和测试高级、复杂的应用程序，这些应用程序在基于英特尔® 处理器的高性能计算 (HPC) 集群上表现更好。</p>
<h3 id="icc"><a href="#icc" class="headerlink" title="icc"></a>icc</h3><p>​    ICC：全称Intel C++ Compiler，是Intel开发的C/C++/Fortran编译器套装，适用于Linux、Microsoft和Mac OS X操作系统，没有非IA指令集版本（就是说仅供x86架构CPU使用）。ICC广泛应用于高性能计算、分布式计算等商业计算领域，其向量化和并行化性能是业界的标杆，能够充分发挥现代处理器的特性。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/07/13/e4ARB.png"></p>
<h1 id="7月14日"><a href="#7月14日" class="headerlink" title="7月14日"></a>7月14日</h1><h2 id="大纲-3"><a href="#大纲-3" class="headerlink" title="大纲"></a>大纲</h2><ol>
<li>并行计算</li>
<li>多线程（openmp）</li>
</ol>
<h2 id="并行计算"><a href="#并行计算" class="headerlink" title="并行计算"></a>并行计算</h2><p>​    最简单的话来解释并行计算就是同时使用 多个计算资源 （就是多个CPU）去运行程序来解决一个需要大量计算的问题 。<br>并行计算程序运行在一个多核心（或多CPU）的计算机、或者由多台计算机组成的网络上。<br>需要进行大量计算的问题，需要被分解成多个独立的、能够同时运行的部分。<br>每个部分将来还将继续分解成一串独立的命令执行流（instructions 命令执行流，也可以是线程）。<br>不同部分的命令执行流可以同时在不同的CPU上得到执行。 换句话说，在同一时间可以有多个线程在执行。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://ask.qcloudimg.com/http-save/developer-news/5prbqmjrgv.jpeg?imageView2/2/w/1620" alt="img"></p>
<h2 id="多线程（openmp）"><a href="#多线程（openmp）" class="headerlink" title="多线程（openmp）"></a>多线程（openmp）</h2><p>操作系统能够进行运算调度的最小单位。一个进程中可以并发多个线程，每条线程并行执行不同的任务。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic3.zhimg.com/80/v2-64919602407124d2c79a09e58f3f8dfe_720w.jpg" alt="img"></p>
<p>如果是多核CPU 处理 CPU 密集型程序，我们完全可以最大化的利用 CPU 核心数，应用并发编程来提高效率</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic3.zhimg.com/80/v2-25f763818b2faa77d7ed0eb7e88b353a_720w.jpg" alt="img"></p>
<p>线程等待时间所占比例越高，需要越多线程；线程CPU时间所占比例越高，需要越少线程。</p>
<p>CPU 密集型程序  -&gt;  数量一般会设置为 <code>CPU 核数（逻辑）+ 1</code>(确保在CPU周期不会中断工作。)</p>
<p>I/O密集型程序  -&gt;  最佳线程数 = CPU核心数 * (1/CPU利用率) =  CPU核心数 *[ 1 + (I/O耗时/CPU耗时)]</p>
<p>最小化临界区范围，因为临界区的大小往往就是瓶颈问题的所在。</p>
<p>OpenMP是：一种应用程序接口(API)，可用于显式指导多线程、共享内存的并行性。它的特点是<strong>动态并行</strong>：在代码的一个部分和另一个部分之间，并行运行的执行流的数量可以变化。</p>
<p><strong>OpenMP编程模型</strong></p>
<p>内存共享模型：OpenMP是专为多处理器/核，共享内存机器所设计的。底层架构可以是UMA和NUMA。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img2018.cnblogs.com/blog/1365470/201812/1365470-20181212223751481-2040405150.png" alt="img"></p>
<p>　<strong>基于线程的并行性</strong></p>
<ul>
<li>OpenMP仅通过线程来完成并行</li>
<li>线程们存在于单个进程的资源中，没有了这个进程，线程也不存在了</li>
<li>通常，线程数与机器的处理器/核数相匹配，然而，实际使用取决与应用程序</li>
<li>OpenMP是一种显式（非自动）编程模型，为程序员提供对并行化的完全控制</li>
<li>并行区域中的所有线程可以同时访问这个共享的数据</li>
</ul>
<p>OpenMP采用fork-join的执行模式。开始的时候只存在一个主线程，当需要进行并行计算的时候，派生出若干个分支线程来执行并行任务。当并行代码执行完成之后，分支线程会合，并把控制流程交给单独的主线程。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img-blog.csdnimg.cn/2019103012094258.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgxOTQ1Mg==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;omp.h&quot;</span>   <span class="comment">//#incluce &quot;&quot;格式：引用非标准库的头文件，编译器从用户的工作目录开始搜索</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>* argv[])</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> nthreads, tid;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">#<span class="meta-keyword">pragma</span> omp parallel private(nthreads, tid) <span class="comment">//&#123;  花括号写在这会报错</span></span></span><br><span class="line">    &#123;  </span><br><span class="line">        tid = omp_get_thread_num();</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Hello World from OMP thread %d\n&quot;</span>, tid);</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(tid == <span class="number">0</span>) &#123;</span><br><span class="line">            nthreads = omp_get_num_threads();</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;Number of threads %d\n&quot;</span>, nthreads);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">g++ filename.cpp -o filename -fopenmp</span><br><span class="line"></span><br><span class="line">./filename</span><br></pre></td></tr></table></figure>
<p>输出:</p>
<blockquote>
<p>Hello World from OMP thread 12<br>Hello World from OMP thread 37<br>Hello World from OMP thread 13<br>Hello World from OMP thread 7<br>Hello World from OMP thread 0<br>Number of threads 40<br>Hello World from OMP thread 39<br>Hello World from OMP thread 2<br>Hello World from OMP thread 5<br>Hello World from OMP thread 38<br>Hello World from OMP thread 22<br>Hello World from OMP thread 4<br>Hello World from OMP thread 10<br>Hello World from OMP thread 17<br>Hello World from OMP thread 3<br>Hello World from OMP thread 26<br>Hello World from OMP thread 1<br>Hello World from OMP thread 8<br>Hello World from OMP thread 35<br>Hello World from OMP thread 14<br>Hello World from OMP thread 6<br>Hello World from OMP thread 18<br>Hello World from OMP thread 28<br>Hello World from OMP thread 20<br>Hello World from OMP thread 11<br>Hello World from OMP thread 9<br>Hello World from OMP thread 23<br>Hello World from OMP thread 33<br>Hello World from OMP thread 34<br>Hello World from OMP thread 31<br>Hello World from OMP thread 36<br>Hello World from OMP thread 19<br>Hello World from OMP thread 21<br>Hello World from OMP thread 29<br>Hello World from OMP thread 15<br>Hello World from OMP thread 25<br>Hello World from OMP thread 24<br>Hello World from OMP thread 30<br>Hello World from OMP thread 16<br>Hello World from OMP thread 27<br>Hello World from OMP thread 32</p>
</blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/weixin_41890971/article/details/86766492">Intel编译器安装OpenMPI4.0</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/weixin_42819452/article/details/102816640">OpenMP使用详解</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/csdnmicrosoftcsdn/article/details/43277439">使用OpenMP给程序加速</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/liurong_cn/article/details/8229605">openMP的一点使用经验</a></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> a[<span class="number">10</span>] = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>&#125;;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> omp parallel for reduction(+:sum)</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">10</span>;i++)</span><br><span class="line">        sum = sum + a[i];</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span>&lt;&lt;<span class="string">&quot;sum: &quot;</span>&lt;&lt;sum&lt;&lt;<span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用归约(reduction)</p>
<p>不使用的话，当某线程A执行sum = sum + a[i]的同时，另一线程B正好在更新sum，而此时A还在用旧的sum做累加，于是出现了错误。</p>
<p>reduction虽然很方便，但它只支持一些基本操作，比如+,-,*,&amp;,|,&amp;&amp;,||等。使用<code>#pragma omp critical</code>用在一段代码临界区之前，保证每次只有一个OpenMP线程进入（临界区）</p>
<p><code>pragma omp sections</code> 之后的代码块通过 <code>pragma omp section</code> 进一步被分为各个子区段。每个 <code>pragma omp section</code> 块将由一个单独的线程执行。</p>
<h1 id="7月15日"><a href="#7月15日" class="headerlink" title="7月15日"></a>7月15日</h1><h2 id="大纲-4"><a href="#大纲-4" class="headerlink" title="大纲"></a>大纲</h2><ol>
<li>多进程（openmpi）</li>
</ol>
<h2 id="openmpi"><a href="#openmpi" class="headerlink" title="openmpi"></a>openmpi</h2><p><a target="_blank" rel="noopener external nofollow noreferrer" href="http://hmli.ustc.edu.cn/doc/mpi/openmpi-install.htm">Open MPI安装使用</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">wget --no-check-certificate https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-4.1.4.tar.gz</span><br><span class="line"></span><br><span class="line">tar -zxvf openmpi-4.1.4.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> openmpi-4.1.4</span><br><span class="line"></span><br><span class="line">./configure --prefix=/home/Yumj/ifycyu/openmpi</span><br><span class="line">make</span><br><span class="line">make install</span><br><span class="line"></span><br><span class="line">MPI_HOME=/home/Yumj/ifycyu/openmpi</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$&#123;MPI_HOME&#125;</span>/bin:<span class="variable">$PATH</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="variable">$&#123;MPI_HOME&#125;</span>/lib:<span class="variable">$LD_LIBRARY_PATH</span></span><br><span class="line"><span class="built_in">export</span> MANPATH=<span class="variable">$&#123;MPI_HOME&#125;</span>/share/man:<span class="variable">$MANPATH</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> examples</span><br><span class="line">make</span><br><span class="line">mpirun -np 4 hello_c</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/07/15/ecvWb.png"></p>
<h3 id="并行计算机和分布式计算"><a href="#并行计算机和分布式计算" class="headerlink" title="并行计算机和分布式计算"></a>并行计算机和分布式计算</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://images2018.cnblogs.com/blog/946688/201804/946688-20180413220556066-418199688.png" alt="img"></p>
<p>多台计算机经由网络相连。</p>
<p>每个节点的进程无法访问其他节点的内存。</p>
<p><strong>⚠️NOTE</strong></p>
<p>一个进程的不同线程可以访问同一个内存；</p>
<p>不同的进程永远不能访问同一个内存（即使是同一台主机上的不同进程）；</p>
<p><strong>使用MPI的代码的原理:</strong></p>
<ol>
<li>多个进程并行启动。</li>
<li>每个进程运行该程序。</li>
<li>该程序描述了每个过程的作用。</li>
<li>关注过程的同步以及它们之间的数据交换。</li>
</ol>
<h3 id="编译，执行"><a href="#编译，执行" class="headerlink" title="编译，执行"></a>编译，执行</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;mpi.h&gt; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    MPI_Init(&amp;argc, &amp;argv);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Sup.\n&quot;</span>);</span><br><span class="line">    MPI_Finalize();</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mpicc -o source source.c &#x2F;&#x2F;mpicc实际上只是一个对gcc的调用 （见：mpicc -showme）</span><br><span class="line">mpirun -n 2 source</span><br></pre></td></tr></table></figure>
<p>⚠️注意：</p>
<p>-n选项指定要并行运行的进程数量。如果省略-n选项，则启动的进程数取决于机器。</p>
<p>mpirun -host host0,host1 source  //允许指定执行所用的机器：这句的意思是：在host0，host1上各执行一次source。</p>
<p><strong><em>点对点通信</em></strong></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Ssend</span><span class="params">(<span class="keyword">const</span> <span class="keyword">void</span> *buf, <span class="keyword">int</span> count, MPI_Datatype datatype,</span></span></span><br><span class="line"><span class="function"><span class="params">     <span class="keyword">int</span> dest, <span class="keyword">int</span> tag, MPI_Comm comm)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Recv</span><span class="params">(<span class="keyword">void</span> *buf, <span class="keyword">int</span> count, MPI_Datatype datatype,</span></span></span><br><span class="line"><span class="function"><span class="params">     <span class="keyword">int</span> source, <span class="keyword">int</span> tag, MPI_Comm comm, MPI_Status *status)</span></span>;</span><br></pre></td></tr></table></figure>
<p>comm : 沟通器</p>
<p>tag : 信息标签</p>
<p>dest/source : 在该沟通器中的发送者和接受者的等级号</p>
<p>datatype : 所发送的数据的类型</p>
<p>count :发送和接受的数据数量</p>
<p>buf : 发送/接收的数据的开始地址</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://images2018.cnblogs.com/blog/946688/201804/946688-20180416181406079-591301273.png" alt="img"></p>
<p><strong>MPI_Ssend的标签必须与MPI_Recv的标签匹配，否则通信失败。</strong></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;mpi.h&gt; </span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    MPI_Init(&amp;argc, &amp;argv);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> wrank; MPI_Comm_rank(MPI_COMM_WORLD, &amp;wrank);</span><br><span class="line">    <span class="keyword">int</span> witness = <span class="number">0</span>; </span><br><span class="line">    <span class="keyword">if</span> (wrank==<span class="number">2</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> modifier = <span class="number">1</span>;</span><br><span class="line">        MPI_Ssend(&amp;modifier, <span class="number">1</span>, MPI_INT, <span class="number">3</span>, <span class="number">28</span>, MPI_COMM_WORLD);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (wrank==<span class="number">3</span>)</span><br><span class="line">        MPI_Recv(&amp;witness, <span class="number">1</span>, MPI_INT, <span class="number">2</span>, <span class="number">28</span>, MPI_COMM_WORLD,</span><br><span class="line">                MPI_STATUS_IGNORE);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Rang %d, witness %d.\n&quot;</span>, wrank, witness); </span><br><span class="line"></span><br><span class="line">    MPI_Finalize();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">                                              </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/07/16/ecR3a.png"></p>
<p>⚠️补充说明：</p>
<p>只有收到指定标签的全部消息时，接收过程才会退出对MPI_Recv的调用。</p>
<p><strong><em>全局同步：MPI_Barrier</em></strong></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MPI_Barrier(MPI_COMM_WORLD);</span><br></pre></td></tr></table></figure>


<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://images2018.cnblogs.com/blog/946688/201804/946688-20180418005805643-157476245.png" alt="img"></p>
<p>P3最先到达barrier，但是它停下来了等了等后面的P0，P1，P2，然后四个进程都在barrier之后，再次开始出发，执行。</p>
<p><em>MPI_Barrier是确保进程之间同步的唯一方法。特别是，随后提出的集体通信都不会确保同步。</em></p>
<p><strong>广播 : MPI_Bcast</strong></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Bcast</span><span class="params">(<span class="keyword">void</span> *buffer, <span class="keyword">int</span> count, MPI_Datatype datatype, <span class="keyword">int</span> root, MPI_Comm comm)</span></span>;</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://images2018.cnblogs.com/blog/946688/201804/946688-20180418023946248-535627891.png" alt="img"></p>
<p>发送者：一个root，唯一的      接受者：包括发送者在内的所有人</p>
<p><strong>选择性分配：MPI_Scatter</strong></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Scatter</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">     <span class="keyword">const</span> <span class="keyword">void</span> *sendbuf, <span class="keyword">int</span> sendcount, MPI_Datatype sendtype,</span></span></span><br><span class="line"><span class="function"><span class="params">     <span class="keyword">void</span> *recvbuf, <span class="keyword">int</span> recvcount, MPI_Datatype recvtype,<span class="keyword">int</span> root, MPI_Comm comm)</span></span>;</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://images2018.cnblogs.com/blog/946688/201804/946688-20180418025459852-1113538277.png" alt="img"></p>
<p>所有进程均须调用该方法， 唯一的发送者root， 接受者为所有进程。 </p>
<p>sendcount是发送到单个进程的数据的数量。MPI_Scatter剪切要发送的数据（与通信器中的进程一样多），大小为sendcount。发送的数据的顺序对应于接收过程的顺序：第i个数据集被发送到等级i的过程。发送的数据总数为sendcount *（通信器中的进程数）。</p>
<p><strong>收集：MPI_Gather</strong></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Gather</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">     <span class="keyword">const</span> <span class="keyword">void</span> *sendbuf, <span class="keyword">int</span> sendcount, MPI_Datatype sendtype,</span></span></span><br><span class="line"><span class="function"><span class="params">     <span class="keyword">void</span> *recvbuf, <span class="keyword">int</span> recvcount, MPI_Datatype recvtype, <span class="keyword">int</span> root, MPI_Comm comm)</span></span>;</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://images2018.cnblogs.com/blog/946688/201804/946688-20180418030527875-1176225741.png" alt="img"></p>
<p>这是MPI_Scatter的逆操作。   root是唯一的接收器，所有进程都是发射器。</p>
<p><strong>普遍收集 MPI_Allgather</strong></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Allgather</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">     <span class="keyword">const</span> <span class="keyword">void</span> *sendbuf, <span class="keyword">int</span> sendcount, MPI_Datatype sendtype,</span></span></span><br><span class="line"><span class="function"><span class="params">     <span class="keyword">void</span> *recvbuf, <span class="keyword">int</span> recvcount, MPI_Datatype recvtype,</span></span></span><br><span class="line"><span class="function"><span class="params">     MPI_Comm comm)</span></span>;</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://images2018.cnblogs.com/blog/946688/201804/946688-20180419150708811-2097656129.png" alt="img"></p>
<p><strong>选择性分配：MPI_Scatterv</strong></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Scatterv</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">     <span class="keyword">const</span> <span class="keyword">void</span> *sendbuf, <span class="keyword">const</span> <span class="keyword">int</span> sendcounts[], <span class="keyword">const</span> <span class="keyword">int</span> displs[],MPI_Datatype sendtype, </span></span></span><br><span class="line"><span class="function"><span class="params">    <span class="keyword">void</span> *recvbuf, <span class="keyword">int</span> recvcount, MPI_Datatype recvtype, <span class="keyword">int</span> root, MPI_Comm comm)</span></span>; </span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://images2018.cnblogs.com/blog/946688/201804/946688-20180419151259702-1966599177.png" alt="img"></p>
<p>给每个进程发送的数据量：sendcounts = {1, 2, 1, 2}</p>
<p>给每个进程发送的数据在缓存中的位置：displs = {0, 1, 3, 4}.</p>
<p><strong>选择性收集 : MPI_Gatherv</strong></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">MPI_Gatherv</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">     <span class="keyword">const</span> <span class="keyword">void</span> *sendbuf, <span class="keyword">int</span> sendcount, MPI_Datatype sendtype,</span></span></span><br><span class="line"><span class="function"><span class="params">     <span class="keyword">void</span> *recvbuf, <span class="keyword">const</span> <span class="keyword">int</span> recvcounts[], <span class="keyword">const</span> <span class="keyword">int</span> displs[],</span></span></span><br><span class="line"><span class="function"><span class="params">     MPI_Datatype recvtype, <span class="keyword">int</span> root, MPI_Comm comm)</span></span>;</span><br></pre></td></tr></table></figure>


<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://images2018.cnblogs.com/blog/946688/201804/946688-20180419153157277-2037463740.png" alt="img"></p>
<h1 id="7月16日"><a href="#7月16日" class="headerlink" title="7月16日"></a>7月16日</h1><h2 id="大纲-5"><a href="#大纲-5" class="headerlink" title="大纲"></a>大纲</h2><ol>
<li>MPI并行程序</li>
</ol>
<h2 id="MPI并行程序"><a href="#MPI并行程序" class="headerlink" title="MPI并行程序"></a>MPI并行程序</h2><p>矩阵分块乘法</p>
<p>将一个矩阵每一列分配给各个子进程，然后在进程内部将一列与另一个矩阵相乘，最后将根进程收集结果并进行输出。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//生成随机矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> **<span class="title">generate_matrix</span><span class="params">(<span class="keyword">int</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> num = <span class="number">0</span>,m;</span><br><span class="line">    <span class="keyword">int</span> **matrix;</span><br><span class="line">    matrix = (<span class="keyword">int</span> **)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">int</span> *) * size); <span class="comment">//二维数组</span></span><br><span class="line">    <span class="keyword">for</span>(m = <span class="number">0</span>; m &lt; size; m++)</span><br><span class="line">        matrix[m] = (<span class="keyword">int</span> *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">int</span>) * size); <span class="comment">//一维数组</span></span><br><span class="line">    <span class="keyword">int</span> i,j;</span><br><span class="line">    srand(time(<span class="literal">NULL</span>) + rand());</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; size; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; size; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            matrix[i][j]= rand() % <span class="number">20</span>;<span class="comment">//填充数字</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> matrix;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//输出矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print_matrx</span><span class="params">(<span class="keyword">int</span> **a,<span class="keyword">int</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i,j;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; size; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; size; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;%d &quot;</span>,a[i][j]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//矩阵相乘</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> * <span class="title">Multiplication</span><span class="params">(<span class="keyword">int</span> **a,<span class="keyword">int</span> b[],<span class="keyword">int</span> size)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> *result;</span><br><span class="line">    result = (<span class="keyword">int</span> *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">int</span>) * size);</span><br><span class="line">    <span class="keyword">int</span> i,m,n,sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(m = <span class="number">0</span>; m &lt; size; m++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(n = <span class="number">0</span>; n &lt; size; n++)</span><br><span class="line">        &#123;</span><br><span class="line">            sum += a[n][m] * b[n];</span><br><span class="line">        &#125;</span><br><span class="line">        result[m] = sum;</span><br><span class="line">        sum = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc,<span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> size,rank,dest;</span><br><span class="line">    MPI_Comm comm = MPI_COMM_WORLD;</span><br><span class="line">    MPI_Status status;</span><br><span class="line">    MPI_Init(&amp;argc,&amp;argv);<span class="comment">//并行初始化函数</span></span><br><span class="line">    MPI_Comm_size(comm,&amp;size);<span class="comment">//获得总的进程数目</span></span><br><span class="line">    MPI_Comm_rank(comm,&amp;rank);<span class="comment">//获得本进程的ID</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> **matrix1;</span><br><span class="line">    <span class="keyword">int</span> **matrix2;</span><br><span class="line">    <span class="keyword">int</span> send_buff[size*size];</span><br><span class="line">    matrix1 = generate_matrix(size);</span><br><span class="line">    matrix2 = generate_matrix(size);</span><br><span class="line">    <span class="keyword">if</span>(rank == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;matrix1 is :\n&quot;</span>);</span><br><span class="line">        print_matrx((<span class="keyword">int</span> **)matrix1,size);</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;matrix2 is :\n&quot;</span>);</span><br><span class="line">        print_matrx((<span class="keyword">int</span> **)matrix2,size);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> j,k,tmp = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; size; j++)</span><br><span class="line">            <span class="keyword">for</span>(k = <span class="number">0</span>; k &lt; size; k++)</span><br><span class="line">            &#123;</span><br><span class="line">                send_buff[tmp] = matrix1[j][k];</span><br><span class="line">                tmp++;</span><br><span class="line">            &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> rbuf[size];</span><br><span class="line">    <span class="keyword">int</span> final_buff[size];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> *result;</span><br><span class="line"></span><br><span class="line">    result = (<span class="keyword">int</span> *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">int</span>) * size);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    MPI_Scatter(send_buff,size,</span><br><span class="line">                ,rbuf,size,MPI_INT,<span class="number">0</span>,comm);<span class="comment">//进程0,分发行</span></span><br><span class="line"></span><br><span class="line">    result = Multiplication((<span class="keyword">int</span> **)matrix2,rbuf,size);<span class="comment">//计算</span></span><br><span class="line">    MPI_Barrier(comm);<span class="comment">//等待所有进程计算结束</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> *recv_buff;</span><br><span class="line">    <span class="keyword">if</span>(rank == <span class="number">0</span>)</span><br><span class="line">        recv_buff = (<span class="keyword">int</span>*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">int</span>)*size*size); </span><br><span class="line">    MPI_Barrier(comm);<span class="comment">//等待所有进程计算结束</span></span><br><span class="line"></span><br><span class="line">    MPI_Gather(result,size,MPI_INT,recv_buff,size,MPI_INT,<span class="number">0</span>,comm);<span class="comment">//收集各列数据</span></span><br><span class="line">    <span class="comment">//根进程进行输出(将二维数组一维化了,)</span></span><br><span class="line">    <span class="keyword">if</span>(rank == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;\nresult is :\n&quot;</span>);</span><br><span class="line">        <span class="keyword">int</span> m,n,tmp = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(m = <span class="number">0</span>; m &lt; size; m++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span>(n = <span class="number">0</span>;n &lt; size;n++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;%d &quot;</span>,recv_buff[tmp]);</span><br><span class="line">                tmp++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    MPI_Finalize();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>






<h1 id="7月17日"><a href="#7月17日" class="headerlink" title="7月17日"></a>7月17日</h1><h2 id="大纲-6"><a href="#大纲-6" class="headerlink" title="大纲"></a>大纲</h2><p>对这一周的内容进行巩固、完善</p>
<h1 id="7月18日"><a href="#7月18日" class="headerlink" title="7月18日"></a>7月18日</h1><h2 id="大纲-7"><a href="#大纲-7" class="headerlink" title="大纲"></a>大纲</h2><ol>
<li>CUDA编程模型</li>
</ol>
<h2 id="CUDA编程模型"><a href="#CUDA编程模型" class="headerlink" title="CUDA编程模型"></a>CUDA编程模型</h2><p>CUDA是一种通用的并行计算平台和编程模型，是在C语言基础上扩展的。</p>
<p>CUDA体系结构的组成来说，包含了三个部分：开发库、运行期环境和驱动</p>
<p>驱动程序必须在每个节点上安装 运行时库可以在共享目录中安装一份</p>
<p>一个典型的CUDA程序是按这样的步骤执行的：</p>
<ol>
<li>把数据从CPU内存拷贝到GPU内存。</li>
<li>调用核函数对存储在GPU内存中的数据进行操作的。</li>
<li>将数据从GPU内存传送回CPU内存。</li>
</ol>
<p>​    尽量使用寄存器，尽量将数据声明为局部变量。而当存在着数据的重复利用时，可以把数据存放在共享内存里。而对于全局内存，我们需要注意用一种合理的方式来进行数据的合并访问，以尽量减少设备对内存子系统再次发出访问操作的次数。</p>
<h3 id="线程管理"><a href="#线程管理" class="headerlink" title="线程管理"></a>线程管理</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic1.zhimg.com/80/v2-927e0893292ddecec2983977a5806c6c_720w.jpg" alt="img"></p>
<p>一个内核启动所产生的所有线程统称一个网格（Grid），同一网格中的所有线程共享相同的全局内存空间。</p>
<p>一个网格由多个线程块（Block）构成。再下一级，一个线程块由一组线程（Thread）构成。</p>
<h3 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic4.zhimg.com/80/v2-087a0ee8e19464285816fefd0970e4b3_720w.jpg" alt="img"></p>
<p>寄存器是GPU上运行速度最快的内存空间，核函数中声明的一个没有其他修饰符的自变量，通常就存储在寄存器中。</p>
<p>共享内存是GPU上可受用户控制的一级缓存。当存在着数据的重复利用时，使用共享内存是比较合适的。</p>
<p>全局内存是GPU中最大、延迟最高并且最常使用的内存。</p>
<p>CUDA给编程者提供了这些可以操作的GPU内存层次结构，这对我们进行数据移动和布局提供了更多可控制的支持，方便了我们以更接近底层硬件实现的思路优化程序，以达到更高的性能。这也是CUDA编程不同于CPU编程的特点之一。</p>
<h1 id="7月19日"><a href="#7月19日" class="headerlink" title="7月19日"></a>7月19日</h1><h2 id="大纲-8"><a href="#大纲-8" class="headerlink" title="大纲"></a>大纲</h2><ol>
<li>GPU硬件结构</li>
<li>软硬件组织结构对比</li>
</ol>
<h2 id="GPU硬件结构"><a href="#GPU硬件结构" class="headerlink" title="GPU硬件结构"></a>GPU硬件结构</h2><p>GPU并不是一个独立运行的计算平台，而需要与CPU协同工作，可以看成是CPU的协处理器，因此当我们在说GPU并行计算时，其实是指的基于CPU+GPU的异构计算架构。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic3.zhimg.com/80/v2-df49a98a67c5b8ce55f1a9afcf21d982_720w.jpg" alt="img"></p>
<p>GPU包括更多的运算核心，其特别适合数据并行的计算密集型任务，CPU负责处理逻辑复杂的串行程序，而GPU重点处理数据密集型的并行计算程序，从而发挥最大功效。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic2.zhimg.com/80/v2-6456af75530956da6bc5bab7418ff9e5_720w.jpg" alt="img"></p>
<p>GPU实际上是一个SM的阵列，每个SM包含N个计算核，GPU的整体结构，其主要是由大量的SM（Streaming-Multiprocessor 流式多处理器）和DRAM存储等构成的。SM由大量计算核（有时也称SP或CUDA核）、LDU（Load-Store Units）、SFU（Special-Function Units）、寄存器、共享内存等构成。这种结构正是GPU具有高并行度计算能力的基础。</p>
<p>当启动一个内核网络时，它的线程块会被分布在可用的SM上来执行。当线程块一旦被调度到一个SM上，其中的线程只会在那个指定的SM上并发执行。多个线程块可能会被分配到同一个SM上，而且是根据SM资源的可用性进行调度的。</p>
<h2 id="软硬件组织结构对比"><a href="#软硬件组织结构对比" class="headerlink" title="软硬件组织结构对比"></a>软硬件组织结构对比</h2><p>CUDA采用单指令多线程（SIMT）架构来管理和执行线程，每32个线程为一组，被称为线程束（Warp）。线程束中所有线程同时执行相同的指令。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic3.zhimg.com/80/v2-7ba17fc91e5feead9a3a6dbfba956c8e_720w.jpg" alt="img"></p>
<p>左侧是逻辑视图从线程构成线程块再构成线程网络。对应右侧的硬件就是CUDA core、SM、GPU。</p>
<p>共享内存被分配在SM上的常驻线程块中，寄存器在线程中被分配。线程块中的线程通过这些资源可以进行相互的合作和通信。尽管线程块里的所有线程都可以逻辑地并行运行，但并不是所有线程都可以同时在物理层面执行。因此线程块里的不同线程可能会以不同速度前进。我们可以使用CUDA语句在需要的时候进行线程的同步。</p>
<h1 id="7月20日"><a href="#7月20日" class="headerlink" title="7月20日"></a>7月20日</h1><h2 id="大纲-9"><a href="#大纲-9" class="headerlink" title="大纲"></a>大纲</h2><ol>
<li>了解基本的CUDA语句及用法</li>
</ol>
<p><small><small>8月2日也是有关cuda</small></small></p>
<h2 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h2><ol>
<li><p>device上分配内存的cudaMalloc函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMalloc</span><span class="params">(<span class="keyword">void</span>** devPtr, <span class="keyword">size_t</span> size)</span></span>;</span><br></pre></td></tr></table></figure></li>
<li><p>负责host和device之间数据通信的cudaMemcpy函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMemcpy</span><span class="params">(<span class="keyword">void</span>* dst, <span class="keyword">const</span> <span class="keyword">void</span>* src, <span class="keyword">size_t</span> count, cudaMemcpyKind kind)</span></span></span><br></pre></td></tr></table></figure>
<p>src指向数据源，而dst是目标区域，count是复制的字节数，其中kind控制复制的方向(cpu-&gt;gpu,gpu-&gt;cpu)</p>
</li>
<li><p>cudaMallocManaged函数分配托管内存</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMallocManaged</span><span class="params">(<span class="keyword">void</span> **devPtr, <span class="keyword">size_t</span> size, <span class="keyword">unsigned</span> <span class="keyword">int</span> flag=<span class="number">0</span>)</span></span>;</span><br></pre></td></tr></table></figure>


</li>
</ol>
<h2 id="向量加法实例"><a href="#向量加法实例" class="headerlink" title="向量加法实例"></a>向量加法实例</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 两个向量加法kernel，grid和block均为一维</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">add</span><span class="params">(<span class="keyword">float</span>* x, <span class="keyword">float</span> * y, <span class="keyword">float</span>* z, <span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 获取全局索引</span></span><br><span class="line">    <span class="keyword">int</span> index = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    <span class="comment">// 步长</span></span><br><span class="line">    <span class="keyword">int</span> stride = blockDim.x * gridDim.x;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = index; i &lt; n; i += stride)</span><br><span class="line">    &#123;</span><br><span class="line">        z[i] = x[i] + y[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> N = <span class="number">1</span> &lt;&lt; <span class="number">20</span>;</span><br><span class="line">    <span class="keyword">int</span> nBytes = N * <span class="keyword">sizeof</span>(<span class="keyword">float</span>);</span><br><span class="line">    <span class="comment">// 申请host内存</span></span><br><span class="line">    <span class="keyword">float</span> *x, *y, *z;</span><br><span class="line">    x = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    y = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    z = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化数据</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        x[i] = <span class="number">10.0</span>;</span><br><span class="line">        y[i] = <span class="number">20.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 申请device内存</span></span><br><span class="line">    <span class="keyword">float</span> *d_x, *d_y, *d_z;</span><br><span class="line">    cudaMalloc((<span class="keyword">void</span>**)&amp;d_x, nBytes);</span><br><span class="line">    cudaMalloc((<span class="keyword">void</span>**)&amp;d_y, nBytes);</span><br><span class="line">    cudaMalloc((<span class="keyword">void</span>**)&amp;d_z, nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将host数据拷贝到device</span></span><br><span class="line">    cudaMemcpy((<span class="keyword">void</span>*)d_x, (<span class="keyword">void</span>*)x, nBytes, cudaMemcpyHostToDevice);</span><br><span class="line">    cudaMemcpy((<span class="keyword">void</span>*)d_y, (<span class="keyword">void</span>*)y, nBytes, cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="comment">// 定义kernel的执行配置</span></span><br><span class="line">    <span class="function">dim3 <span class="title">blockSize</span><span class="params">(<span class="number">256</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">gridSize</span><span class="params">((N + blockSize.x - <span class="number">1</span>) / blockSize.x)</span></span>;</span><br><span class="line">    <span class="comment">// 执行kernel</span></span><br><span class="line">    add &lt;&lt; &lt; gridSize, blockSize &gt;&gt; &gt;(d_x, d_y, d_z, N);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将device得到的结果拷贝到host</span></span><br><span class="line">    cudaMemcpy((<span class="keyword">void</span>*)z, (<span class="keyword">void</span>*)d_z, nBytes, cudaMemcpyDeviceToHost);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 检查执行结果</span></span><br><span class="line">    <span class="keyword">float</span> maxError = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; i++)</span><br><span class="line">        maxError = fmax(maxError, <span class="built_in">fabs</span>(z[i] - <span class="number">30.0</span>));</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;最大误差: &quot;</span> &lt;&lt; maxError &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 释放device内存</span></span><br><span class="line">    cudaFree(d_x);</span><br><span class="line">    cudaFree(d_y);</span><br><span class="line">    cudaFree(d_z);</span><br><span class="line">    <span class="comment">// 释放host内存</span></span><br><span class="line">    <span class="built_in">free</span>(x);</span><br><span class="line">    <span class="built_in">free</span>(y);</span><br><span class="line">    <span class="built_in">free</span>(z);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic3.zhimg.com/80/v2-e4258399e69394620daa1e199d53fe0e_720w.jpg" alt="img"></p>
<p>需要单独在host和device上进行内存分配，并且要进行数据拷贝，这是很容易出错的。好在CUDA 6.0引入统一内存来避免这种麻烦。</p>
<p>用法如下</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> N = <span class="number">1</span> &lt;&lt; <span class="number">20</span>;</span><br><span class="line">    <span class="keyword">int</span> nBytes = N * <span class="keyword">sizeof</span>(<span class="keyword">float</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 申请托管内存</span></span><br><span class="line">    <span class="keyword">float</span> *x, *y, *z;</span><br><span class="line">    cudaMallocManaged((<span class="keyword">void</span>**)&amp;x, nBytes);</span><br><span class="line">    cudaMallocManaged((<span class="keyword">void</span>**)&amp;y, nBytes);</span><br><span class="line">    cudaMallocManaged((<span class="keyword">void</span>**)&amp;z, nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化数据</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        x[i] = <span class="number">10.0</span>;</span><br><span class="line">        y[i] = <span class="number">20.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定义kernel的执行配置</span></span><br><span class="line">    <span class="function">dim3 <span class="title">blockSize</span><span class="params">(<span class="number">256</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">gridSize</span><span class="params">((N + blockSize.x - <span class="number">1</span>) / blockSize.x)</span></span>;</span><br><span class="line">    <span class="comment">// 执行kernel</span></span><br><span class="line">    add &lt;&lt; &lt; gridSize, blockSize &gt;&gt; &gt;(x, y, z, N);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 同步device 保证结果能正确访问</span></span><br><span class="line">    cudaDeviceSynchronize();</span><br><span class="line">    <span class="comment">// 检查执行结果</span></span><br><span class="line">    <span class="keyword">float</span> maxError = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; i++)</span><br><span class="line">        maxError = fmax(maxError, <span class="built_in">fabs</span>(z[i] - <span class="number">30.0</span>));</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;最大误差: &quot;</span> &lt;&lt; maxError &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 释放内存</span></span><br><span class="line">    cudaFree(x);</span><br><span class="line">    cudaFree(y);</span><br><span class="line">    cudaFree(z);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h1 id="7月21日"><a href="#7月21日" class="headerlink" title="7月21日"></a>7月21日</h1><h2 id="大纲-10"><a href="#大纲-10" class="headerlink" title="大纲"></a>大纲</h2><ol>
<li>矩阵乘法实例</li>
</ol>
<h2 id="矩阵乘法实例"><a href="#矩阵乘法实例" class="headerlink" title="矩阵乘法实例"></a>矩阵乘法实例</h2><ol>
<li><p>结构体:</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 矩阵类型，行优先，M(row, col) = *(M.elements + row * M.width + col)</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Matrix</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">int</span> width;</span><br><span class="line">    <span class="keyword">int</span> height;</span><br><span class="line">    <span class="keyword">float</span> *elements;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></li>
<li><p>矩阵乘法核函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取矩阵A的(row, col)元素</span></span><br><span class="line"><span class="function">__device__ <span class="keyword">float</span> <span class="title">getElement</span><span class="params">(Matrix *A, <span class="keyword">int</span> row, <span class="keyword">int</span> col)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">return</span> A-&gt;elements[row * A-&gt;width + col];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 为矩阵A的(row, col)元素赋值</span></span><br><span class="line"><span class="function">__device__ <span class="keyword">void</span> <span class="title">setElement</span><span class="params">(Matrix *A, <span class="keyword">int</span> row, <span class="keyword">int</span> col, <span class="keyword">float</span> value)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	A-&gt;elements[row * A-&gt;width + col] = value;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 矩阵相乘kernel，2-D，每个线程计算一个元素</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">matMulKernel</span><span class="params">(Matrix *A, Matrix *B, Matrix *C)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">float</span> Cvalue = <span class="number">0.0</span>;</span><br><span class="line">	<span class="keyword">int</span> row = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">	<span class="keyword">int</span> col = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; A-&gt;width; ++i)</span><br><span class="line">	&#123;</span><br><span class="line">		Cvalue += getElement(A, row, i) * getElement(B, i, col);</span><br><span class="line">	&#125;</span><br><span class="line">	setElement(C, row, col, Cvalue);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



</li>
<li><p>使用统一内存</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> width = <span class="number">1</span> &lt;&lt; <span class="number">10</span>;</span><br><span class="line">    <span class="keyword">int</span> height = <span class="number">1</span> &lt;&lt; <span class="number">10</span>;</span><br><span class="line">    Matrix *A, *B, *C;</span><br><span class="line">    <span class="comment">// 申请托管内存</span></span><br><span class="line">    cudaMallocManaged((<span class="keyword">void</span>**)&amp;A, <span class="keyword">sizeof</span>(Matrix));</span><br><span class="line">    cudaMallocManaged((<span class="keyword">void</span>**)&amp;B, <span class="keyword">sizeof</span>(Matrix));</span><br><span class="line">    cudaMallocManaged((<span class="keyword">void</span>**)&amp;C, <span class="keyword">sizeof</span>(Matrix));</span><br><span class="line">    <span class="keyword">int</span> nBytes = width * height * <span class="keyword">sizeof</span>(<span class="keyword">float</span>);</span><br><span class="line">    cudaMallocManaged((<span class="keyword">void</span>**)&amp;A-&gt;elements, nBytes);</span><br><span class="line">    cudaMallocManaged((<span class="keyword">void</span>**)&amp;B-&gt;elements, nBytes);</span><br><span class="line">    cudaMallocManaged((<span class="keyword">void</span>**)&amp;C-&gt;elements, nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化数据</span></span><br><span class="line">    A-&gt;height = height;</span><br><span class="line">    A-&gt;width = width;</span><br><span class="line">    B-&gt;height = height;</span><br><span class="line">    B-&gt;width = width;</span><br><span class="line">    C-&gt;height = height;</span><br><span class="line">    C-&gt;width = width;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; width * height; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        A-&gt;elements[i] = <span class="number">1.0</span>;</span><br><span class="line">        B-&gt;elements[i] = <span class="number">2.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定义kernel的执行配置(二维线程块32×32 维线程网格128×128)</span></span><br><span class="line">    <span class="function">dim3 <span class="title">blockSize</span><span class="params">(<span class="number">32</span>, <span class="number">32</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">gridSize</span><span class="params">((width + blockSize.x - <span class="number">1</span>) / blockSize.x, </span></span></span><br><span class="line"><span class="function"><span class="params">        (height + blockSize.y - <span class="number">1</span>) / blockSize.y)</span></span>;</span><br><span class="line">    <span class="comment">// 将核函数放在线程网格中执行</span></span><br><span class="line">    matMulKernel &lt;&lt; &lt; gridSize, blockSize &gt;&gt; &gt;(A, B, C);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 同步device 保证结果能正确访问</span></span><br><span class="line">    cudaDeviceSynchronize();</span><br><span class="line">    <span class="comment">// 检查执行结果</span></span><br><span class="line">    <span class="keyword">float</span> maxError = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; width * height; ++i)</span><br><span class="line">        maxError = fmax(maxError, <span class="built_in">fabs</span>(C-&gt;elements[i] - <span class="number">2</span> * width));</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">&quot;最大误差: &quot;</span> &lt;&lt; maxError &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


</li>
</ol>
<h1 id="7月22日"><a href="#7月22日" class="headerlink" title="7月22日"></a>7月22日</h1><h2 id="大纲-11"><a href="#大纲-11" class="headerlink" title="大纲"></a>大纲</h2><ol>
<li>UM(Unified Memory)</li>
</ol>
<h2 id="什么是UM"><a href="#什么是UM" class="headerlink" title="什么是UM"></a>什么是UM</h2><p>在今天典型的PC上，CPU与GPU的内存是物理上独立的，通过PCI-E总线进行连接通信。实际上，在CUDA 6.0之前，程序员必须在编程期间很清楚这一点，并且反应在代码中。必须在CPU和GPU两端都进行内存分配，并不断地进行手动copy，来保证两端的内存一致。</p>
<p>Unified memory在程序员的视角中，维护了一个统一的内存池，在CPU与GPU中共享。使用了单一指针进行托管内存，由系统来自动地进行内存迁移。</p>
<p><code>CPU代码（左）与带有UM的CUDA代码（右）</code></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">sortfile</span><span class="params">(FILE *fp, <span class="keyword">int</span> N)</span>             <span class="keyword">void</span> <span class="title">sortfile</span><span class="params">(FILE *fp, <span class="keyword">int</span> N)</span>                   </span></span><br><span class="line"><span class="function"></span>&#123;                                          &#123;</span><br><span class="line">    <span class="keyword">char</span> *data;                                <span class="keyword">char</span> *data; </span><br><span class="line">    data = (<span class="keyword">char</span>*)<span class="built_in">malloc</span>(N);                   cudaMallocManaged(data, N);</span><br><span class="line"></span><br><span class="line">    fread(data, <span class="number">1</span>, N, fp);                     fread(data, <span class="number">1</span>, N, fp);</span><br><span class="line"></span><br><span class="line">    qsort(data, N, <span class="number">1</span>, compare);                qsort&lt;&lt;&lt;...&gt;&gt;&gt;(data, N, <span class="number">1</span>, compare);</span><br><span class="line">                                               cudaDeviceSynchronize();</span><br><span class="line"></span><br><span class="line">    usedata(data);                             usedata(data);</span><br><span class="line">    <span class="built_in">free</span>(data);                                <span class="built_in">free</span>(data);</span><br><span class="line">&#125;                                          &#125;</span><br></pre></td></tr></table></figure>


<ul>
<li>简化了代码编写和内存模型</li>
<li><ul>
<li>可以在CPU端和GPU端共用一个指针，不用单独各自分配空间。方便管理，减少了代码量。</li>
<li>更方便的代码迁移。</li>
</ul>
</li>
<li>链表，本质上是有指针组成的嵌套的数据结构，内存空间的传递很复杂。<ul>
<li>在CPU和GPU间直接传递链表元素。</li>
<li>在CPU或GPU任一一端来修改链表元素。</li>
<li>避免了复杂的同步问题。</li>
</ul>
</li>
</ul>
<h1 id="7月23日"><a href="#7月23日" class="headerlink" title="7月23日"></a>7月23日</h1><h2 id="大纲-12"><a href="#大纲-12" class="headerlink" title="大纲"></a>大纲</h2><ol>
<li>了解vtune、nvprof</li>
</ol>
<h2 id="Intel-VTune-Profiler"><a href="#Intel-VTune-Profiler" class="headerlink" title="Intel VTune Profiler"></a>Intel VTune Profiler</h2><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/yaojingqingcheng/article/details/120335335">使用Intel VTune Profiler进行性能分析及优化</a></p>
<p>Intel VTune Profiler是一个全平台的性能分析工具，可以帮助你快速发现和分析应用程序及整个系统的性能瓶颈。</p>
<p>用户获取的信息主要有以下几点：</p>
<ol>
<li>确定占用大量处理器时间的区域（热点）；</li>
<li>查看应用运行过程中的情况（通过检测绑定process）；</li>
<li>没有充分的有效利用可用的处理器时间的代码；</li>
<li>影响应用程序性能的同步对象；</li>
<li>应用程序是否浪费时间在输入输出操作上，在哪里浪费的，为何浪费；</li>
<li>对比不同的同步方法、不同的线程数量、不同的算法之间对性能的影响；</li>
<li>线程的活动状态和状态的转换；</li>
<li>代码中与硬件相关的性能瓶颈（如缓存缺失、分支预测失败等）；</li>
</ol>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">int</span> *a = (<span class="keyword">int</span> *)<span class="built_in">malloc</span>(<span class="number">1024</span> * <span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; <span class="number">1024</span>; i ++)</span><br><span class="line">        a[i] = i + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img-blog.csdnimg.cn/bf7e88b1cb674ae6a5c36c3d1ec8cab4.png" alt="img"></p>
<p>分析可知，有内存泄漏的风险</p>
<h2 id="nvprof"><a href="#nvprof" class="headerlink" title="nvprof"></a>nvprof</h2><p> nvprof 是一个可用于 Linux 、 Windows 和 OS X 的命令行探查器。</p>
<p>通过使用 <code>nvprof ./myApp</code> 运行应用程序，可以快速看到它所使用的所有内核和内存副本的摘要</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#x3D;&#x3D;9261&#x3D;&#x3D; Profiling application: .&#x2F;tHogbomCleanHemi</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;9261&#x3D;&#x3D; Profiling result:</span><br><span class="line"></span><br><span class="line">  Time(%)      Time     Calls       Avg       Min       Max  Name</span><br><span class="line"></span><br><span class="line">   58.73%  737.97ms      1000  737.97us  424.77us  1.1405ms  subtractPSFLoop_kernel(float const *, int, float*, int, int, int, int, int, int, int, float, float)</span><br><span class="line"></span><br><span class="line">   38.39%  482.31ms      1001  481.83us  475.74us  492.16us  findPeakLoop_kernel(MaxCandidate*, float const *, int)</span><br><span class="line"></span><br><span class="line">    1.87%  23.450ms         2  11.725ms  11.721ms  11.728ms  [CUDA memcpy HtoD]</span><br><span class="line"></span><br><span class="line">    1.01%  12.715ms      1002  12.689us  2.1760us  10.502ms  [CUDA memcpy DtoH]</span><br></pre></td></tr></table></figure>


<p>摘要将对同一内核的所有调用组合在一起，显示每个内核的总时间和总应用程序时间的百分比。</p>
<h1 id="7月24日"><a href="#7月24日" class="headerlink" title="7月24日"></a>7月24日</h1><h2 id="大纲-13"><a href="#大纲-13" class="headerlink" title="大纲"></a>大纲</h2><p>对这一周的内容进行巩固、完善，回顾 linux 系统下的命令行操作的知识。</p>
<h1 id="7月25-26日"><a href="#7月25-26日" class="headerlink" title="7月25-26日"></a>7月25-26日</h1><h2 id="大纲-14"><a href="#大纲-14" class="headerlink" title="大纲"></a>大纲</h2><ol>
<li>InfiniBand</li>
<li>InfiniBand测试工具</li>
</ol>
<h2 id="InfiniBand"><a href="#InfiniBand" class="headerlink" title="InfiniBand"></a>InfiniBand</h2><p>[InfiniBand简介](<a target="_blank" rel="noopener external nofollow noreferrer" href="http://www.asc-events.net/ASC22/v/pdf1/ASC22">http://www.asc-events.net/ASC22/v/pdf1/ASC22</a> -  InfiniBand简介.pdf)</p>
<p>InfiniBand绕过网络堆栈，为两端应用程序之间的通信创建直接通道，从而避免了操作系统的介入</p>
<p>InfiniBand是为硬件实现而设计的，与TCP不同的是，TCP的体系结构考虑的是软件实现。</p>
<p>InfiniBand架构是一种支持多并发的“转换线缆”技术，它是新一代服务器硬件平台的I/O标准。由于它具有高带宽、低延时、 高可扩展性的特点</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/08/02/AMHPC.png"></p>
<p>其组成单元主要分为四类：</p>
<ol>
<li>HCA（Host Channel Adapter），它是连接内存控制器和TCA的桥梁；将RDMA网卡称为HCA</li>
<li>Infiniband link，它是连接HCA的光纤</li>
<li>交换机和路由器<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/08/02/AoFTL.png"></li>
</ol>
<p>​    发送数据时,会检测接受者是否有buf(防止丢包) 无损网络,不需要像TCP窗口算法那样的丢包机制来确定最佳的正在传输的数据包数量。这使得高效的产品能够以极低的延迟和可忽略的CPU使用率为应用程序提供56 GB/s的数据速率。</p>
<p>建立虚拟通道,获取远端应用数据(不用os网络协议栈进行数据拷贝)</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/08/02/AoicX.png"></p>
<p>通过网口,cpu不感知</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/08/02/Ao1Ot.png"></p>
<p>RDMA是一种通过网络在应用程序之间直接传输数据的能力，无需操作系统的参与，同时消耗双方可忽略的CPU资源（零拷贝传输）。CPU完全不感知，直接内存访问，解放CPU</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/08/02/AoQyb.png"></p>
<p>硬件卸载架构 1让CPU告知网卡,网卡聚合,CPU计算不被打断</p>
<h2 id="测试工具"><a href="#测试工具" class="headerlink" title="测试工具"></a>测试工具</h2><p>ibv_devinfo 显示device信息</p>
<p>ibdev2netdev -v 查看网口映射关系</p>
<p>ibstatus -d xxx 查看xxx设备在子网中的状态</p>
<p>ibswitches  查询全网Switch  ibhosts 查询全网hosts</p>
<p>ibtracert  xx xx 两个地址直接通过的switch</p>
<p>nvidia-smi topo -m 查询网卡到gpu拓扑</p>
<h1 id="7月27-29日"><a href="#7月27-29日" class="headerlink" title="7月27-29日"></a>7月27-29日</h1><h2 id="大纲-15"><a href="#大纲-15" class="headerlink" title="大纲"></a>大纲</h2><ol>
<li>MTCNN Pytorch实现</li>
</ol>
<h2 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h2><p>论文:</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/ftp/arxiv/papers/1604/1604.02878.pdf">Face Detection</a></p>
<p>代码:</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/xuexingyu24/MTCNN_Tutorial">MTCNN_Tutorial</a></p>
<p>数据集:</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://aistudio.baidu.com/aistudio/datasetdetail/4336">WIDER FACE数据集</a>的wider_face_split.zip与WIDER_train.zip压缩包，解压后存放路径分别为：<br>MTCNN_TUTORIAL-MASTER/data_set/wider_face_split/<br>MTCNN_TUTORIAL-MASTER/data_set/WIDER_train/</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="http://mmlab.ie.cuhk.edu.hk/archive/CNN_FacePoint.htm">CNN for Facial Point Detection )</a>下载Training set,，解压后存放路径为：</p>
<p>MTCNN_TUTORIAL-MASTER/data_set/landmark/</p>
<h2 id="代码运行"><a href="#代码运行" class="headerlink" title="代码运行"></a>代码运行</h2><h3 id="图像标注"><a href="#图像标注" class="headerlink" title="图像标注"></a>图像标注</h3><ol>
<li><p>运行<code> data_preprocessing/transform.py</code>,将.mat标记文件转换为txt格式</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/07/31/rfWVv.png"></p>
</li>
<li><p>运行<code> data_preprocessing/gen_Pnet_train_data.py</code>小于0.3划分为Negatives，0.4-0.65划分为Part faces，大于0.65划分为Positives。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/07/31/rfkqq.png"></p>
</li>
<li><p>运行<code> data_preprocessing/assemble_Pnet_imglist.py</code> 组装PNet的数据集注释文件并将其打乱，完成PNet训练数据的准备。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/07/31/rfxrc.png"></p>
</li>
<li><p>运行<code> train/Train_Pnet.py</code>，训练PNet。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/07/31/rfzQr.png"></p>
</li>
<li><p>运行<code>data_preprocessing/gen_Rnet_train_data.py</code>  <code>/data_preprocessing/assemble_Rnet_imglist.py</code> 生成RNet训练数据</p>
</li>
<li><p>运行<code>train/Train_Rnet.py</code> 训练RNet</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/07/31/rfSnt.png"></p>
</li>
<li><p>运行<code> data_preprocessing/gen_Onet_train_data.py</code>  <code>data_preprocessing/gen_landmark_48.py</code> <code>data_preprocessing/assemble_Onet_imglist.py</code>生成ONet训练数据</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/08/01/rXHPg.png"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/08/01/rXtub.png"></p>
</li>
<li><p>运行<code>train/Train_Onet.py</code>，训练ONet</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/08/01/rXL9B.png"></p>
</li>
</ol>
<p>训练,验证阶段完成</p>
<h2 id="结果测试"><a href="#结果测试" class="headerlink" title="结果测试:"></a>结果测试:</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/07/30/rQm01.png"></p>
<p>找了很多教程，一直无法显示图片</p>
<p>linux干啥要cv.imshow()，于是改为保存图片</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/07/30/rQpwI.png"></p>
<p>成功保存</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/07/30/rQP1D.jpg"></p>
<h2 id="ps"><a href="#ps" class="headerlink" title="ps"></a>ps</h2><p>有些文件的路径有问题,需要换成绝对路径才能运行</p>
<p>sys.path.append(‘..’)有时候换成sys.path.append(‘.’)才能运行</p>
<h1 id="7月30日"><a href="#7月30日" class="headerlink" title="7月30日"></a>7月30日</h1><h2 id="大纲-16"><a href="#大纲-16" class="headerlink" title="大纲"></a>大纲</h2><ol>
<li>AI Software Technology</li>
</ol>
<h2 id="英特尔至强可扩展处理器"><a href="#英特尔至强可扩展处理器" class="headerlink" title="英特尔至强可扩展处理器"></a>英特尔至强可扩展处理器</h2><p>发展趋势 : 核数增多 向量位宽越高</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/08/04/AkqDX.png"></p>
<p>向量化和多线程同时引入,性能大幅度提升 </p>
<p>DL boost</p>
<p>VNNI 是一个专门用来对 int 8 低精度模型的<strong>推理</strong>进行加速的一个指令。</p>
<p>由于采用了这种更低精度,这个模型它的模型会变小。运算的时候不光是需要更少的计算资源，同时它也会占用更少的内存带宽，从而使得在一台一路服务器上，在一台服务器上可以同时并发地去运行多路的推理请求，从而使得整体的性能大幅提升。</p>
<h2 id="代码的优化"><a href="#代码的优化" class="headerlink" title="代码的优化"></a>代码的优化</h2><ol>
<li><p>第一个部分: performance 的一个分析，需要知道我这个代码到底哪些部分是最耗时的。</p>
</li>
<li><p>第二个部分: 根据这些最耗时的这些代码去分析它到底是因为它的多线程没有做好，亦或是它的向量化做得不是很好，或者是说它的一个 IO 是它的一个瓶颈，需要找出它的瓶颈。</p>
<ol>
<li>编译器选项</li>
<li>提升内存的访问效率 内存的对齐，包括合适的预取</li>
<li>向量化和并行化。 </li>
</ol>
</li>
</ol>
<p>单节点上进行优化的一些方法</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/08/04/Axde6.png"></p>
<ol>
<li>基本的一些优化，<ol>
<li>编译的选项O0， O1 是不要选的，O0的话通常是做 debug 去用的。建议 O2 或者O3。O2 和O3取决于应用程序。</li>
<li>尽量能够用到一些已经优化的数学库，比如说 intel 的 mkr 矩阵库</li>
<li>选择合适的精度。</li>
</ol>
</li>
<li>内存的访问</li>
<li>SIMD<ol>
<li>提升它的一个向量化的效率，使它尽量地去跟它的理论分值能够达到匹配</li>
</ol>
</li>
<li>并行化<ol>
<li>充分地利用一个多核的优势，使得多核的性能能够达到一个线性的加速</li>
<li>不要频繁地去创建各种的 threads 因为创建了以后再把它 delay 的时候也会有一些开销。</li>
</ol>
</li>
</ol>
<p>单节点上性能已经达到最优，并不是说在多节点上一定达到最优</p>
<p>如果IO量非常大的时候，可能会需要考虑将串行的 IO 去改成并行的 IO 做到直接多个节点之间的一个 load balance 包括这种异步的通讯等等。</p>
<h1 id="8月1日"><a href="#8月1日" class="headerlink" title="8月1日"></a>8月1日</h1><h2 id="大纲-17"><a href="#大纲-17" class="headerlink" title="大纲"></a>大纲</h2><ol>
<li>NGC简介</li>
<li>工具包</li>
<li>Demo</li>
</ol>
<h2 id="NGC是什么"><a href="#NGC是什么" class="headerlink" title="NGC是什么"></a>NGC是什么</h2><p>​    借助NGC 目录（NVIDIA 的 GPU 优化 AI 和高性能计算软件中心）中的生产就绪型 AI 预训练模型，数据科学家和开发者可以快速适应模型，或直接将模型按原样部署以进行推理。</p>
<p>​    NGC:合理分配资源，使得所有的开发者都能够很快的去完成她自己的这部分开发工作，而且能够去减轻自己的管理和维护的负担。</p>
<ol>
<li>成套的软件和工具能够去覆盖到从数据的采集整理标注再到模型的训练，到模型的推理优化，以及再到构建完整的 AI 应用的各个方面。</li>
<li>随着 AI 的发展，需要越来越大的 AI 的模型和越来越多的训练数据。有更强大的性能能够去适应越来越大的一个算力的需求。</li>
<li>软件和工具能够在不同的平台上都经过了充分的测试。不管我们是面对一个单卡单GPU的场景还是多卡甚至是多机多卡多节点的一个场景，还是说在云上，在边缘端，在本地服务器等等不同的地方都具备一个很好的稳定性和可靠性。</li>
</ol>
<h2 id="工具包"><a href="#工具包" class="headerlink" title="工具包"></a>工具包</h2><h3 id="TAO-迁移学习"><a href="#TAO-迁移学习" class="headerlink" title="TAO(迁移学习)"></a>TAO(迁移学习)</h3><p>在基本不写代码或者是很少量的写代码的这个情况下，去使用数据以及 ngc 上提供的预训练的模型</p>
<h3 id="RIVA-对话式"><a href="#RIVA-对话式" class="headerlink" title="RIVA(对话式)"></a>RIVA(对话式)</h3><p>具备了世界一流的这个语音识别和文本转语音的模型。</p>
<h3 id="DEEPSTREAM"><a href="#DEEPSTREAM" class="headerlink" title="DEEPSTREAM"></a>DEEPSTREAM</h3><p>​    实现了一个完整的视频分析的pipeline ，它的源头支持多种视频流的格式和视频数据的来源。实现了从视频流的解码，然后以及视频的前处理后处理的工作。也提供了 plugin 的工具，可以去帮助客户来做定制化的工作。</p>
<h3 id="CLARA"><a href="#CLARA" class="headerlink" title="CLARA"></a>CLARA</h3><p>帮医生来做医疗影像的辅助诊断，来帮助医生去识别器官或者组织中的一些疾病。</p>
<h2 id="demo"><a href="#demo" class="headerlink" title="demo"></a>demo</h2><p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://catalog.ngc.nvidia.com/orgs/nvidia/collections/imagesegmentation">Image Segmentation | NVIDIA NGC</a></p>
<p>拉取docker</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker pull nvcr.io/nvidia/pytorch:20.10-py3</span><br><span class="line">docker run --gpus all -it --rm nvcr.io/nvidia/pytorch:20.10-py3</span><br></pre></td></tr></table></figure>
<p>开始训练:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /workspace/examples/maskrcnn/pytorch</span><br><span class="line">mkdir /datasets </span><br><span class="line">ln –sf &lt;/workspace/downloaded/coco&gt; /datasets/data</span><br><span class="line">python -m torch.distributed.launch --nproc_per_node=<span class="number">8</span> tools/train_net.py --config-file configs/e2e_mask_rcnn_R_50_FPN_1x.yaml DTYPE <span class="string">&quot;float16&quot;</span> </span><br></pre></td></tr></table></figure>
<h1 id="8月2-3日"><a href="#8月2-3日" class="headerlink" title="8月2-3日"></a>8月2-3日</h1><h2 id="大纲-18"><a href="#大纲-18" class="headerlink" title="大纲"></a>大纲</h2><ol>
<li>CUDA</li>
</ol>
<p>感觉之前学的cuda内容不太完善</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://face2ai.com/program-blog/#GPU%E7%BC%96%E7%A8%8B%EF%BC%88CUDA%EF%BC%89">人工智能编程 | 谭升的博客 (face2ai.com)</a></p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>​     GPU 有加速并行的优势，就可以把一个原来的应用程序中可并行的部分把它移植到 GPU 上来让它做计算。然后其它的关于逻辑相关的控制依然放在 CPU 端，进而实现整个应用的性能的提升。</p>
<p>CPU和GPU线程的区别：</p>
<ol>
<li>CPU线程是重量级实体，操作系统交替执行线程，线程上下文切换花销很大</li>
<li>GPU线程是轻量级的，GPU应用一般包含成千上万的线程，多数在排队状态，线程之间切换基本没有开销。</li>
<li>CPU的核被设计用来尽可能减少一个或两个线程运行时间的延迟，而GPU核则是大量线程，最大幅度提高吞吐量</li>
</ol>
<p>​    CUDA是英伟达推出的一个并行计算架构和编程模型</p>
<ol>
<li><p>架构扩大。把 GPU 用于通用计算，然后使得应用程序能够充分利用 CPU 和 GPU 的资源。利用各自的优势来充分地发挥 GPU 在并行计算方面的能力。在 CUDA 编程方面， CUDA 对 CUDA C++编程，其实是对于 C 或者 C++的一个扩展。 CUDA 可以和 CUDA Python 结合，就是对 Python的一个扩展。并且在扩大除了对语言的这种扩展以外，它还会有自己的各种 API 来帮助我们去管理设备和内存。</p>
</li>
<li><p>什么是异构计算呢？ 虽然GPU有很多的核心，功能也很强大，但是它本身是不能独立工作的，也就是说它必须与 CPU 进行协同合作。CUDA并行计算其实就是 CPU 和 GPU 的协同计算，也就是异构计算。然后在CUDA的异构计算中，我们通常把在 CPU 上面执行的代码称为主机端的代码，然后在 GPU 上执行的代码称为设备端的代码。主机端的代码只能够用到主机端的内存。设备端代码只能在它设备上写内存，只能使用它设备上的内存。主机端和设备端内设备端的内存通过 PCIe bus 也就 PCIe 总线来进行连接。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/08/05/AhmsK.png"></p>
</li>
</ol>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/08/05/AhJoa.png"></p>
<h2 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h2><p>__global__关键字，声明这个函数是一个kernel函数(所有执行在设备端的函数都称为 kernel 函数)</p>
<p>声明的这个函数表示这个函数在 GPU 上面是可全局调用的(既能在主机端调用，又能在设备端调用。)，执行是在设备端。<code>返回值类型必须是一个 void 型，并且不支持可变参</code></p>
<p>device 这个关键字，声明的函数它能执行在设备端，并且它的调用也只能在设备端调用。</p>
<p>host 其实就和 device 是对应的，只能执行在主机端。调用也只能在主机端来调用。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HelloFromGpu&lt;&lt;&lt;<span class="number">1</span>,kThreadCount&gt;&gt;&gt;()</span><br></pre></td></tr></table></figure>
<p>这个三个尖括号’&lt;&lt;&lt;grid,block&gt;&gt;&gt;’内是对设备代码执行的线程结构的配置（或者简称为对内核进行配置）</p>
<p>指明kernel launch过程(异步)  1个线程块里面包含5个线程</p>
<p>通过指定grid和block的维度，我们可以配置：</p>
<ul>
<li>内核中线程的数目</li>
<li>内核中使用的线程布局</li>
</ul>
<p>Kernel核函数编写有以下限制</p>
<ol>
<li>只能访问设备内存</li>
<li>必须有void返回类型</li>
<li>不支持可变数量的参数</li>
<li>不支持静态变量</li>
<li>显示异步行为</li>
</ol>
<h2 id="线程管理-1"><a href="#线程管理-1" class="headerlink" title="线程管理"></a>线程管理</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://face2ai.com/CUDA-F-2-0-CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B01/4.png" alt="img"></p>
<p>一个线程块block中的线程可以完成下述协作：</p>
<ul>
<li>同步</li>
<li>共享内存</li>
</ul>
<p><strong>不同块内线程不能相互影响！他们是物理隔离的！</strong></p>
<p>idx = blockIdx.x  * blockDim.x + threadIdx.x</p>
<p>写 kernel 的执行配置的时候，block的维度尽量是 32 的倍数，使得它在执行的时候能够更好地利用资源，避免资源的浪费。</p>
<h2 id="内存管理-1"><a href="#内存管理-1" class="headerlink" title="内存管理"></a>内存管理</h2><p>为达到最优性能，CUDA提供了在主机端准备设备内存的函数，并且显式地向设备传递数据，显式的从设备取回数据。</p>
<p><code>cudaMalloc((void **)&amp;d_a,nbytes);</code> 返回值是错误码</p>
<p>要注意向上取整,防止线程不够导致积分错误 </p>
<p>CUDA6.0 中设备代码不能调用<code>cudaMallocManaged</code>，只能主机调用，所有托管内存必须在主机代码上动态声明，或者全局静态声明</p>
<h2 id="进行扩大编程"><a href="#进行扩大编程" class="headerlink" title="进行扩大编程"></a>进行扩大编程</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/08/05/AiYKK.png"></p>
<h2 id="调试分析"><a href="#调试分析" class="headerlink" title="调试分析"></a>调试分析</h2><p>通过<code> cudaGetLastError(void)</code>获取错误码,通过<code>printf(&quot;%s&quot;,cudaGetErrorString(cudaGetLastError));</code>获取问题</p>
<h1 id="8月4日"><a href="#8月4日" class="headerlink" title="8月4日"></a>8月4日</h1><h2 id="大纲-19"><a href="#大纲-19" class="headerlink" title="大纲"></a>大纲</h2><ol>
<li>pytorch</li>
</ol>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://handbook.pytorch.wiki/">PyTorch 中文手册（pytorch handbook） - Pytorch中文手册</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.zhangxiann.com/">(开篇词)PyTorch 学习笔记 - PyTorch 学习笔记 (zhangxiann.com)</a></p>
<h2 id="pytorch-1"><a href="#pytorch-1" class="headerlink" title="pytorch"></a>pytorch</h2><p>所有的 Tensor 类型默认都是基于CPU，<code>.to</code> 方法 可以将Tensor移动到任何设备中</p>
<p>如果有多个GPU，使用<code>nn.DataParallel</code>来包装我们的模型。 然后通过<code>model.to(device)</code>把模型放到GPU上。</p>
<p>Tensor和numpy对象共享内存，所以他们之间的转换很快，而且几乎不会消耗什么资源。但这也意味着，如果其中一个变了，另外一个也会随之改变。</p>
<h1 id="8月5日"><a href="#8月5日" class="headerlink" title="8月5日"></a>8月5日</h1><h2 id="大纲-20"><a href="#大纲-20" class="headerlink" title="大纲"></a>大纲</h2><ol>
<li>TensorFlow</li>
</ol>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://mp.weixin.qq.com/s?__biz=MzIwODI2NDkxNQ==&mid=2247484633&idx=1&sn=adf2dfee2bf09e6dab0a67d329bd0c50&chksm=97048f65a073067365daa419808913b50872a18ef9bb16a5011f90967eb89c335fb204c027d2&scene=21#wechat_redirect">TensorFlow快速入门资料和翻译</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hands1ml.apachecn.org/">Sklearn 与 TensorFlow 机器学习实用指南第二版</a></p>
<h2 id="TensorFlow的数据类型"><a href="#TensorFlow的数据类型" class="headerlink" title="TensorFlow的数据类型"></a>TensorFlow的数据类型</h2><p>▪ scalar: 1.1  常数</p>
<p>▪ vector: [1.1], [1.1, 2.2, … ]  一维</p>
<p>▪ matrix: [[1.1, 2.2], [3.3, 4.4], [5.5, 6.6]]  二维</p>
<p><code>tf.device(&quot;cpu&quot;)</code>:调用设备</p>
<h2 id="Keras"><a href="#Keras" class="headerlink" title="Keras"></a>Keras</h2><p>模型的<code>summary()</code>方法可以展示所有层</p>
<p>可以用<code>get_weights()</code>和<code>set_weights()</code>方法，获取神经层的所有参数。</p>
<p>创建好模型之后，必须调用<code>compile()</code>方法，设置损失函数和优化器，编译模型</p>
<p>只需调用<code>fit()</code>方法，训练模型</p>
<p><code>fit()</code>方法会返回<code>History</code>对象，包含：训练参数（<code>history.params</code>）、周期列表（<code>history.epoch</code>）、以及最重要的包含训练集和验证集的每个周期后的损失和指标的字典（<code>history.history</code>）。如果用这个字典创建一个 pandas 的<code>DataFrame</code>，然后使用方法<code>plot()</code>，就可以画出学习曲线</p>
<p>Keras 使用 HDF5 格式保存模型架构（包括每层的超参数）和每层的所有参数值（连接权重和偏置项）。还保存了优化器（包括超参数和状态）。</p>
<h2 id="TensorBoard可视化"><a href="#TensorBoard可视化" class="headerlink" title="TensorBoard可视化"></a>TensorBoard可视化</h2><p>TensorBoard 是一个强大的交互可视化工具，使用它可以查看训练过程中的学习曲线、比较每次运行的学习曲线、可视化计算图、分析训练数据、查看模型生成的图片、可视化投射到 3D 的多维数据，等等。TensorBoard 是 TensorFlow 自带的。</p>
<p>要使用 TensorBoard，必须修改程序，将要可视化的数据输出为二进制的日志文件<code>event files</code>。每份二进制数据称为摘要<code>summary</code>，TensorBoard 服务器会监测日志文件目录，自动加载更新并可视化：这样就能看到实时数据（稍有延迟），比如训练时的学习曲线。通常，将 TensorBoard 服务器指向根日志目录，程序的日志写入到它的子目录，这样一个 TensorBoard 服务就能可视化并比较多次运行的数据，而不会将其搞混</p>
<h1 id="8月6日"><a href="#8月6日" class="headerlink" title="8月6日"></a>8月6日</h1><h2 id="大纲-21"><a href="#大纲-21" class="headerlink" title="大纲"></a>大纲</h2><ol>
<li>Intel oneAPI</li>
<li>oneAPI 工具包</li>
</ol>
<h2 id="Intel-oneAPI"><a href="#Intel-oneAPI" class="headerlink" title="Intel oneAPI"></a>Intel oneAPI</h2><p>Intel oneAPI 是一个跨行业、开放、基于标准的统一的<strong>编程模型</strong>，它为跨 CPU、GPU、FPGA、专用加速器的开发者提供统一的体验</p>
<p>具体来说，<strong>OneAPI的核心是一个名为Data Parallel C++（DPC++）的编程语言</strong>。DPC++本质上是C++的扩展，增加了对SYCL的支持。</p>
<p>oneAPI主要分为两部分：</p>
<ol>
<li><p>跨框架的编程语言。它基于行业标准和开放规范，支持广泛的行业生态系统采纳该技术来推动应用开发领域的新演进。 </p>
</li>
<li><p>跨框架库的集合。它相对应的支持直接编程和API编程，通过统一的语言和库，在包括CPU、GPU、FPGA等不同硬件上，提供完整的本地代码性能。</p>
</li>
</ol>
<h2 id="oneAPI-工具包"><a href="#oneAPI-工具包" class="headerlink" title="oneAPI 工具包"></a>oneAPI 工具包</h2><h3 id="Intel®-oneAPI-Base-Toolkit"><a href="#Intel®-oneAPI-Base-Toolkit" class="headerlink" title="Intel® oneAPI Base Toolkit"></a>Intel® oneAPI Base Toolkit</h3><p>这个工具包是 oneAPI 其他产品的基础，包含了几个我们在 Parallel Studio中常用的软件以及 icc 编译器、MPI、DPCPP 等。这个工具包使开发人员都可以跨CPU、GPU和FPGA构建、测试和部署以性能为中心、以数据为中心的应用程序。</p>
<h3 id="Intel®-oneAPI-HPC-Toolkit"><a href="#Intel®-oneAPI-HPC-Toolkit" class="headerlink" title="Intel® oneAPI HPC Toolkit"></a>Intel® oneAPI HPC Toolkit</h3><p>工具包提供可扩展的快速C++、Fortran、OpenMP和MPI应用程序。Intel® oneAPI Base Toolkit和Intel® oneAPI HPC Toolkit几乎包含Intel Parallel Studio XE的功能</p>
<h3 id="Intel®-AI-Analytics-Toolkit"><a href="#Intel®-AI-Analytics-Toolkit" class="headerlink" title="Intel® AI Analytics Toolkit"></a>Intel® AI Analytics Toolkit</h3><p>工具包为数据科学家、AI开发人员和研究人员提供了优化的深度学习框架（PyTorch、TensorFlow等）和高性能Python库，加速端到端机器学习和数据科学库。这些组件是使用oneAPI库构建的，用于低级计算优化。可最大化从预期处理到机器学习的性能。</p>
<h1 id="8月7日"><a href="#8月7日" class="headerlink" title="8月7日"></a>8月7日</h1><h2 id="大纲-22"><a href="#大纲-22" class="headerlink" title="大纲"></a>大纲</h2><p>总结前面所学的内容</p>
<h1 id="8月8日"><a href="#8月8日" class="headerlink" title="8月8日"></a>8月8日</h1><h2 id="大纲-23"><a href="#大纲-23" class="headerlink" title="大纲"></a>大纲</h2><ol>
<li>并行计算</li>
<li>集群的构建</li>
</ol>
<h2 id="并行算法"><a href="#并行算法" class="headerlink" title="并行算法"></a>并行算法</h2><p>一般我们就通过性能加速比这个值的大小或者高低来衡量我们这次性能优化的一个效果。</p>
<p>阿姆达尔定律:Speedup = $\frac{1}{S+V/R}$</p>
<p>S:不能并行化的部分<br>V:可以并行化的部分<br>R:并行化改进</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/08/10/ASamb.png"></p>
<p>结论1：R在理论上决定了最大的性能加速。<br>结论2：S限制了可以实现的实际性能加速。</p>
<h2 id="并行计算机体系结构"><a href="#并行计算机体系结构" class="headerlink" title="并行计算机体系结构"></a>并行计算机体系结构</h2><h3 id="指令级并行"><a href="#指令级并行" class="headerlink" title="指令级并行"></a>指令级并行</h3><p>这个可以认为是计算机里面比较小的一类任务，因为它是通过同时执行多条指令来实现的。这个指令在计算机里面是一个非常小的任务了。</p>
<h3 id="数据并行"><a href="#数据并行" class="headerlink" title="数据并行"></a>数据并行</h3><p>有的指令可以同时处理多组数据产生多个结果。那这样的一种并行就叫做数据级并行，它是在指令级并行基础上发展起来的，可以认为是扩充了指令的功能，和指令实际上是在一个层面上的。</p>
<h3 id="线程级的并行"><a href="#线程级的并行" class="headerlink" title="线程级的并行"></a>线程级的并行</h3><p>一组指令构成了一个线程，这个线程可以认为是一个比较简单的任务。</p>
<h1 id="8月9日"><a href="#8月9日" class="headerlink" title="8月9日"></a>8月9日</h1><h2 id="大纲-24"><a href="#大纲-24" class="headerlink" title="大纲"></a>大纲</h2><ol>
<li>编译原理</li>
</ol>
<h2 id="gcc编译的过程"><a href="#gcc编译的过程" class="headerlink" title="gcc编译的过程"></a>gcc编译的过程</h2><p>预处理—编译—汇编—链接</p>
<h2 id="静态链接"><a href="#静态链接" class="headerlink" title="静态链接"></a>静态链接</h2><p>静态链接就是在 <strong>装载之前</strong>，就完成所有的符号引用的一种链接方式。</p>
<p>在生成可执行文件的时候（链接阶段），把所有需要的函数的二进制代码都包含到可执行文件中去。</p>
<p><strong>优点</strong>：</p>
<ol>
<li>在程序发布的时候就不需要依赖库，也就是不再需要带着库一块发布，程序可以独立执行。</li>
</ol>
<p><strong>缺点</strong>：</p>
<ol>
<li>浪费内存空间。在多进程的操作系统下，同一时间，内存中可能存在多个相同的公共库函数。</li>
<li>程序的开发与发布流程受模块制约。 只要有一个模块更新，那么就需要重新编译打包整个代码。</li>
</ol>
<h2 id="动态链接"><a href="#动态链接" class="headerlink" title="动态链接"></a>动态链接</h2><p>将对符号的重定位推迟到程序 <strong>运行时</strong> 才进行。</p>
<p>在编译的时候不直接拷贝可执行代码，而是通过 <strong>记录一系列符号和参数</strong>，在程序运行或加载时将这些信息传递给操作系统，操作系统负责将需要的动态库加载到内存中，然后程序在运行到指定的代码时，去共享执行内存中已经加载的动态库可执行代码，最终达到运行时连接的目的。</p>
<p><strong>优点</strong>： </p>
<ol>
<li>解决了静态链接的缺陷，更适应现代的大规模的软件开发。</li>
</ol>
<p><strong>缺点</strong>：</p>
<ol>
<li><p>结构复杂。</p>
</li>
<li><p>由于是运行时加载，可能会影响程序的前期执行性能。</p>
</li>
</ol>
<h1 id="8月10日"><a href="#8月10日" class="headerlink" title="8月10日"></a>8月10日</h1><h2 id="大纲-25"><a href="#大纲-25" class="headerlink" title="大纲"></a>大纲</h2><ol>
<li>分布式训练</li>
</ol>
<h2 id="数据并行-1"><a href="#数据并行-1" class="headerlink" title="数据并行"></a>数据并行</h2><p>​    将样本数据进行切分，<strong>切分后的数据</strong> 被送至各个训练节点，与 <strong>完整的模型</strong> 进行运算，最后将多个节点的信息进行合并</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/08/11/AcuCx.png" alt="img"></p>
<p>在一个训练的时间间隔内，各个GPU设备可以<strong>并行地用各自分片的数据进行模型训练</strong>，从而大大加速了整体模型的训练。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.imgtg.com/2022/08/11/AcC4p.jpg" alt="img"></p>
<ol>
<li>CPU将数据分配给GPU0和GPU1</li>
<li>不同GPU存储相同的模型，进行前向和反向传播</li>
<li>模型进行权重同步和更新</li>
</ol>
<h2 id="模型并行"><a href="#模型并行" class="headerlink" title="模型并行"></a>模型并行</h2><p>将模型进行切分，<strong>完整的数据</strong> 被送至各个训练节点，与 <strong>切分后的模型</strong> 进行运算，最后将多个节点的运算结果合并</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic3.zhimg.com/80/v2-e36e84636f67594a873745948b89967a_720w.jpg" alt="img"></p>
<p>某些情况下，模型规模特别巨大，参数特别多以至于单个GPU的显存塞不下，于是只能通过<strong>模型并行</strong>的方式进行训练，即将模型的各网络层甚至是某一层的参数矩阵划分至多张GPU上进行训练。</p>
<h2 id="分布式训练和集合通信"><a href="#分布式训练和集合通信" class="headerlink" title="分布式训练和集合通信"></a>分布式训练和集合通信</h2><ol>
<li><p><strong>集合通信库：</strong>用于分布式训练时，多个计算设备之间的集合通信，</p>
</li>
<li><p>数据加载与预处理库：分布式训练需要处理海量数据，这使得单机单卡时代不需要考虑的数据加载问题，在分布式时代很容易成为瓶颈，为此，通常需要对分布式训练中的数据即预处理做相关优化。</p>
</li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://blog.ifycyu.ml">Murmure</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://blog.ifycyu.ml/posts/b19ce5f3.html">http://blog.ifycyu.ml/posts/b19ce5f3.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://blog.ifycyu.ml" target="_blank">Murmure's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/JNU/">JNU</a><a class="post-meta__tags" href="/tags/%E8%B6%85%E7%AE%97/">超算</a></div><div class="post_share"><div class="social-share" data-image="http://p9.itc.cn/images01/20200619/bbf273ea4df24f0b938fd64d2cf039bd.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="/pluginsSrc/butterfly-extsrc/ShareJS/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="/pluginsSrc/butterfly-extsrc/ShareJS/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/posts/5a1a850e.html"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://th.bing.com/th/id/R.1dd27580fef41f45e954f3b5e42de2cf?rik=GU0D4Svisb279w&amp;pid=ImgRaw&amp;r=0" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">JNU_毛概期末</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/5a1a850e.html" title="JNU_毛概期末"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://th.bing.com/th/id/R.1dd27580fef41f45e954f3b5e42de2cf?rik=GU0D4Svisb279w&pid=ImgRaw&r=0" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2022-08-22</div><div class="title">JNU_毛概期末</div></div></a></div><div><a href="/posts/8c2f6b20.html" title="JNU_算法期末"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="http://t15.baidu.com/it/u=2584374022,3178771271&fm=224&app=112&f=JPEG?w=317&h=500" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2022-06-14</div><div class="title">JNU_算法期末</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7%E6%9C%8811%E6%97%A5"><span class="toc-number">2.</span> <span class="toc-text">7月11日</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E7%BA%B2"><span class="toc-number">2.1.</span> <span class="toc-text">大纲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4"><span class="toc-number">2.2.</span> <span class="toc-text">linux基本命令:</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%96%E8%AF%91%E5%99%A8%E7%BC%96%E8%AF%91%E8%BF%87%E7%A8%8B"><span class="toc-number">2.3.</span> <span class="toc-text">编译器编译过程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#make-cmake-makefile-CmakeList-txt"><span class="toc-number">2.4.</span> <span class="toc-text">make cmake makefile CmakeList.txt</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#c%E3%80%81c-%E3%80%81python"><span class="toc-number">2.5.</span> <span class="toc-text">c、c++、python</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8C%87%E4%BB%A4%E9%9B%86%EF%BC%8Cgpu%E6%9E%B6%E6%9E%84"><span class="toc-number">2.6.</span> <span class="toc-text">指令集，gpu架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%87%E4%BB%A4%E9%9B%86"><span class="toc-number">2.6.1.</span> <span class="toc-text">指令集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gpu%E6%9E%B6%E6%9E%84"><span class="toc-number">2.6.2.</span> <span class="toc-text">gpu架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cuda-core"><span class="toc-number">2.6.3.</span> <span class="toc-text">cuda core</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tensor-core"><span class="toc-number">2.6.4.</span> <span class="toc-text">tensor core</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7%E6%9C%8812%E6%97%A5"><span class="toc-number">3.</span> <span class="toc-text">7月12日</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E7%BA%B2-1"><span class="toc-number">3.1.</span> <span class="toc-text">大纲</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#pytorch"><span class="toc-number">3.1.1.</span> <span class="toc-text">pytorch</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#torch-nn"><span class="toc-number">3.1.1.1.</span> <span class="toc-text">torch.nn</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#nn-Linear%E7%B1%BB%EF%BC%88%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%EF%BC%89"><span class="toc-number">3.1.1.1.1.</span> <span class="toc-text">nn.Linear类（全连接层）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#nn-functional%EF%BC%88%E5%B8%B8%E8%A7%81%E5%87%BD%E6%95%B0"><span class="toc-number">3.1.1.1.2.</span> <span class="toc-text">nn.functional（常见函数)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#nn-xxx%E5%92%8Cnn-functional-xxx%E6%AF%94%E8%BE%83"><span class="toc-number">3.1.1.1.3.</span> <span class="toc-text">nn.xxx和nn.functional.xxx比较</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#nn-Module%E7%B1%BB"><span class="toc-number">3.1.1.1.4.</span> <span class="toc-text">nn.Module类</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%B1%BB%EF%BC%88%E7%BB%A7%E6%89%BF%E4%BA%8EModule%E7%B1%BB%EF%BC%89"><span class="toc-number">3.1.1.1.5.</span> <span class="toc-text">自定义神经网络模型类（继承于Module类）</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Dataset"><span class="toc-number">3.1.1.2.</span> <span class="toc-text">Dataset</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Dataloader"><span class="toc-number">3.1.1.3.</span> <span class="toc-text">Dataloader</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7%E6%9C%8813%E6%97%A5"><span class="toc-number">4.</span> <span class="toc-text">7月13日</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E7%BA%B2-2"><span class="toc-number">4.1.</span> <span class="toc-text">大纲:</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%86%E7%BE%A4"><span class="toc-number">4.2.</span> <span class="toc-text">集群</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%96%E8%AF%91%E5%99%A8%E4%BC%98%E5%8C%96"><span class="toc-number">4.3.</span> <span class="toc-text">编译器优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Intel-MPI"><span class="toc-number">4.3.1.</span> <span class="toc-text">Intel-MPI</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#icc"><span class="toc-number">4.3.2.</span> <span class="toc-text">icc</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7%E6%9C%8814%E6%97%A5"><span class="toc-number">5.</span> <span class="toc-text">7月14日</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E7%BA%B2-3"><span class="toc-number">5.1.</span> <span class="toc-text">大纲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97"><span class="toc-number">5.2.</span> <span class="toc-text">并行计算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E7%BA%BF%E7%A8%8B%EF%BC%88openmp%EF%BC%89"><span class="toc-number">5.3.</span> <span class="toc-text">多线程（openmp）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7%E6%9C%8815%E6%97%A5"><span class="toc-number">6.</span> <span class="toc-text">7月15日</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E7%BA%B2-4"><span class="toc-number">6.1.</span> <span class="toc-text">大纲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#openmpi"><span class="toc-number">6.2.</span> <span class="toc-text">openmpi</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%92%8C%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97"><span class="toc-number">6.2.1.</span> <span class="toc-text">并行计算机和分布式计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E8%AF%91%EF%BC%8C%E6%89%A7%E8%A1%8C"><span class="toc-number">6.2.2.</span> <span class="toc-text">编译，执行</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7%E6%9C%8816%E6%97%A5"><span class="toc-number">7.</span> <span class="toc-text">7月16日</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E7%BA%B2-5"><span class="toc-number">7.1.</span> <span class="toc-text">大纲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MPI%E5%B9%B6%E8%A1%8C%E7%A8%8B%E5%BA%8F"><span class="toc-number">7.2.</span> <span class="toc-text">MPI并行程序</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7%E6%9C%8817%E6%97%A5"><span class="toc-number">8.</span> <span class="toc-text">7月17日</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E7%BA%B2-6"><span class="toc-number">8.1.</span> <span class="toc-text">大纲</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7%E6%9C%8818%E6%97%A5"><span class="toc-number">9.</span> <span class="toc-text">7月18日</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E7%BA%B2-7"><span class="toc-number">9.1.</span> <span class="toc-text">大纲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CUDA%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="toc-number">9.2.</span> <span class="toc-text">CUDA编程模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E7%A8%8B%E7%AE%A1%E7%90%86"><span class="toc-number">9.2.1.</span> <span class="toc-text">线程管理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86"><span class="toc-number">9.2.2.</span> <span class="toc-text">内存管理</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7%E6%9C%8819%E6%97%A5"><span class="toc-number">10.</span> <span class="toc-text">7月19日</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E7%BA%B2-8"><span class="toc-number">10.1.</span> <span class="toc-text">大纲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GPU%E7%A1%AC%E4%BB%B6%E7%BB%93%E6%9E%84"><span class="toc-number">10.2.</span> <span class="toc-text">GPU硬件结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BD%AF%E7%A1%AC%E4%BB%B6%E7%BB%84%E7%BB%87%E7%BB%93%E6%9E%84%E5%AF%B9%E6%AF%94"><span class="toc-number">10.3.</span> <span class="toc-text">软硬件组织结构对比</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7%E6%9C%8820%E6%97%A5"><span class="toc-number">11.</span> <span class="toc-text">7月20日</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E7%BA%B2-9"><span class="toc-number">11.1.</span> <span class="toc-text">大纲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80"><span class="toc-number">11.2.</span> <span class="toc-text">基础</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E5%8A%A0%E6%B3%95%E5%AE%9E%E4%BE%8B"><span class="toc-number">11.3.</span> <span class="toc-text">向量加法实例</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7%E6%9C%8821%E6%97%A5"><span class="toc-number">12.</span> <span class="toc-text">7月21日</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E7%BA%B2-10"><span class="toc-number">12.1.</span> <span class="toc-text">大纲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E5%AE%9E%E4%BE%8B"><span class="toc-number">12.2.</span> <span class="toc-text">矩阵乘法实例</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7%E6%9C%8822%E6%97%A5"><span class="toc-number">13.</span> <span class="toc-text">7月22日</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E7%BA%B2-11"><span class="toc-number">13.1.</span> <span class="toc-text">大纲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFUM"><span class="toc-number">13.2.</span> <span class="toc-text">什么是UM</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7%E6%9C%8823%E6%97%A5"><span class="toc-number">14.</span> <span class="toc-text">7月23日</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E7%BA%B2-12"><span class="toc-number">14.1.</span> <span class="toc-text">大纲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Intel-VTune-Profiler"><span class="toc-number">14.2.</span> <span class="toc-text">Intel VTune Profiler</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#nvprof"><span class="toc-number">14.3.</span> <span class="toc-text">nvprof</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7%E6%9C%8824%E6%97%A5"><span class="toc-number">15.</span> <span class="toc-text">7月24日</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E7%BA%B2-13"><span class="toc-number">15.1.</span> <span class="toc-text">大纲</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7%E6%9C%8825-26%E6%97%A5"><span class="toc-number">16.</span> <span class="toc-text">7月25-26日</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E7%BA%B2-14"><span class="toc-number">16.1.</span> <span class="toc-text">大纲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#InfiniBand"><span class="toc-number">16.2.</span> <span class="toc-text">InfiniBand</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7"><span class="toc-number">16.3.</span> <span class="toc-text">测试工具</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7%E6%9C%8827-29%E6%97%A5"><span class="toc-number">17.</span> <span class="toc-text">7月27-29日</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E7%BA%B2-15"><span class="toc-number">17.1.</span> <span class="toc-text">大纲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B5%84%E6%BA%90"><span class="toc-number">17.2.</span> <span class="toc-text">资源</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E8%BF%90%E8%A1%8C"><span class="toc-number">17.3.</span> <span class="toc-text">代码运行</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E6%A0%87%E6%B3%A8"><span class="toc-number">17.3.1.</span> <span class="toc-text">图像标注</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C%E6%B5%8B%E8%AF%95"><span class="toc-number">17.4.</span> <span class="toc-text">结果测试:</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ps"><span class="toc-number">17.5.</span> <span class="toc-text">ps</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7%E6%9C%8830%E6%97%A5"><span class="toc-number">18.</span> <span class="toc-text">7月30日</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E7%BA%B2-16"><span class="toc-number">18.1.</span> <span class="toc-text">大纲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%8B%B1%E7%89%B9%E5%B0%94%E8%87%B3%E5%BC%BA%E5%8F%AF%E6%89%A9%E5%B1%95%E5%A4%84%E7%90%86%E5%99%A8"><span class="toc-number">18.2.</span> <span class="toc-text">英特尔至强可扩展处理器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E7%9A%84%E4%BC%98%E5%8C%96"><span class="toc-number">18.3.</span> <span class="toc-text">代码的优化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8%E6%9C%881%E6%97%A5"><span class="toc-number">19.</span> <span class="toc-text">8月1日</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E7%BA%B2-17"><span class="toc-number">19.1.</span> <span class="toc-text">大纲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NGC%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">19.2.</span> <span class="toc-text">NGC是什么</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B7%A5%E5%85%B7%E5%8C%85"><span class="toc-number">19.3.</span> <span class="toc-text">工具包</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#TAO-%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0"><span class="toc-number">19.3.1.</span> <span class="toc-text">TAO(迁移学习)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RIVA-%E5%AF%B9%E8%AF%9D%E5%BC%8F"><span class="toc-number">19.3.2.</span> <span class="toc-text">RIVA(对话式)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DEEPSTREAM"><span class="toc-number">19.3.3.</span> <span class="toc-text">DEEPSTREAM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CLARA"><span class="toc-number">19.3.4.</span> <span class="toc-text">CLARA</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#demo"><span class="toc-number">19.4.</span> <span class="toc-text">demo</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8%E6%9C%882-3%E6%97%A5"><span class="toc-number">20.</span> <span class="toc-text">8月2-3日</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E7%BA%B2-18"><span class="toc-number">20.1.</span> <span class="toc-text">大纲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">20.2.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="toc-number">20.3.</span> <span class="toc-text">核函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E7%A8%8B%E7%AE%A1%E7%90%86-1"><span class="toc-number">20.4.</span> <span class="toc-text">线程管理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86-1"><span class="toc-number">20.5.</span> <span class="toc-text">内存管理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%9B%E8%A1%8C%E6%89%A9%E5%A4%A7%E7%BC%96%E7%A8%8B"><span class="toc-number">20.6.</span> <span class="toc-text">进行扩大编程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B0%83%E8%AF%95%E5%88%86%E6%9E%90"><span class="toc-number">20.7.</span> <span class="toc-text">调试分析</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8%E6%9C%884%E6%97%A5"><span class="toc-number">21.</span> <span class="toc-text">8月4日</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E7%BA%B2-19"><span class="toc-number">21.1.</span> <span class="toc-text">大纲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch-1"><span class="toc-number">21.2.</span> <span class="toc-text">pytorch</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8%E6%9C%885%E6%97%A5"><span class="toc-number">22.</span> <span class="toc-text">8月5日</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E7%BA%B2-20"><span class="toc-number">22.1.</span> <span class="toc-text">大纲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TensorFlow%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-number">22.2.</span> <span class="toc-text">TensorFlow的数据类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Keras"><span class="toc-number">22.3.</span> <span class="toc-text">Keras</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">22.4.</span> <span class="toc-text">TensorBoard可视化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8%E6%9C%886%E6%97%A5"><span class="toc-number">23.</span> <span class="toc-text">8月6日</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E7%BA%B2-21"><span class="toc-number">23.1.</span> <span class="toc-text">大纲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Intel-oneAPI"><span class="toc-number">23.2.</span> <span class="toc-text">Intel oneAPI</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#oneAPI-%E5%B7%A5%E5%85%B7%E5%8C%85"><span class="toc-number">23.3.</span> <span class="toc-text">oneAPI 工具包</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Intel%C2%AE-oneAPI-Base-Toolkit"><span class="toc-number">23.3.1.</span> <span class="toc-text">Intel® oneAPI Base Toolkit</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Intel%C2%AE-oneAPI-HPC-Toolkit"><span class="toc-number">23.3.2.</span> <span class="toc-text">Intel® oneAPI HPC Toolkit</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Intel%C2%AE-AI-Analytics-Toolkit"><span class="toc-number">23.3.3.</span> <span class="toc-text">Intel® AI Analytics Toolkit</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8%E6%9C%887%E6%97%A5"><span class="toc-number">24.</span> <span class="toc-text">8月7日</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E7%BA%B2-22"><span class="toc-number">24.1.</span> <span class="toc-text">大纲</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8%E6%9C%888%E6%97%A5"><span class="toc-number">25.</span> <span class="toc-text">8月8日</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E7%BA%B2-23"><span class="toc-number">25.1.</span> <span class="toc-text">大纲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B9%B6%E8%A1%8C%E7%AE%97%E6%B3%95"><span class="toc-number">25.2.</span> <span class="toc-text">并行算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84"><span class="toc-number">25.3.</span> <span class="toc-text">并行计算机体系结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%87%E4%BB%A4%E7%BA%A7%E5%B9%B6%E8%A1%8C"><span class="toc-number">25.3.1.</span> <span class="toc-text">指令级并行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C"><span class="toc-number">25.3.2.</span> <span class="toc-text">数据并行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E7%A8%8B%E7%BA%A7%E7%9A%84%E5%B9%B6%E8%A1%8C"><span class="toc-number">25.3.3.</span> <span class="toc-text">线程级的并行</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8%E6%9C%889%E6%97%A5"><span class="toc-number">26.</span> <span class="toc-text">8月9日</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E7%BA%B2-24"><span class="toc-number">26.1.</span> <span class="toc-text">大纲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#gcc%E7%BC%96%E8%AF%91%E7%9A%84%E8%BF%87%E7%A8%8B"><span class="toc-number">26.2.</span> <span class="toc-text">gcc编译的过程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9D%99%E6%80%81%E9%93%BE%E6%8E%A5"><span class="toc-number">26.3.</span> <span class="toc-text">静态链接</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A8%E6%80%81%E9%93%BE%E6%8E%A5"><span class="toc-number">26.4.</span> <span class="toc-text">动态链接</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8%E6%9C%8810%E6%97%A5"><span class="toc-number">27.</span> <span class="toc-text">8月10日</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E7%BA%B2-25"><span class="toc-number">27.1.</span> <span class="toc-text">大纲</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C-1"><span class="toc-number">27.2.</span> <span class="toc-text">数据并行</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C"><span class="toc-number">27.3.</span> <span class="toc-text">模型并行</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E5%92%8C%E9%9B%86%E5%90%88%E9%80%9A%E4%BF%A1"><span class="toc-number">27.4.</span> <span class="toc-text">分布式训练和集合通信</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('http://p9.itc.cn/images01/20200619/bbf273ea4df24f0b938fd64d2cf039bd.png')"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2022 By Murmure</div><div class="footer_custom_text">Hi, welcome to my <a href="http://blog.ifycyu.ml/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="/pluginsSrc/medium-zoom/dist/medium-zoom.min.js"></script><script src="/pluginsSrc/instant.page/instantpage.js" type="module"></script><script src="/pluginsSrc/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="/pluginsSrc/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('/pluginsSrc/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? '' : ''

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('/pluginsSrc/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://twikoo-ochre-two.vercel.app/',
      region: 'us-east-1',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://twikoo-ochre-two.vercel.app/',
      region: 'us-east-1',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('/pluginsSrc/twikoo/dist/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script type="text/javascript" src="/js/jquery.js"></script><script type="text/javascript" src="/js/jquery.min.js"></script><script src="/js/top_bg.js"></script><canvas class="fireworks" mobile="true"></canvas><script src="/pluginsSrc/butterfly-extsrc/dist/fireworks.min.js"></script><link rel="stylesheet" href="/pluginsSrc/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="/pluginsSrc/aplayer/dist/APlayer.min.js"></script><script src="/pluginsSrc/butterfly-extsrc/MetingJS/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" rel="external nofollow noreferrer" style="margin-inline:5px" data-title="博客框架为Hexo_v5.4.0" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" rel="external nofollow noreferrer" style="margin-inline:5px" data-title="主题版本Butterfly_v4.3.0" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" alt=""/></a><a class="github-badge" target="_blank" href="https://vercel.com/" rel="external nofollow noreferrer" style="margin-inline:5px" data-title="本站部署托管于Vercel" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-Vercel-brightgreen?style=flat&amp;logo=Vercel" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" rel="external nofollow noreferrer" style="margin-inline:5px" data-title="本站项目由Github托管" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" alt=""/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=Claris" alt=""/></a></p>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><!-- hexo injector body_end end --></body></html>